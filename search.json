[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "my-tech-notebook",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nDiffusion Model\n\n\n\nDeep Learning\n\nMath\n\n\n\n\n\n\n\n\n\nJan 9, 2026\n\n\nYuyang\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/diffusion-model/index.html",
    "href": "posts/diffusion-model/index.html",
    "title": "Diffusion Model",
    "section": "",
    "text": "Diffusion models are a class of image generative neural networks inspired by and formulated as a simulation of the thermodynamic diffusion process. The core idea is to model the data generation process as a gradual denoising process, where a neural network learns to reverse a diffusion process that progressively adds noise to the data until it becomes pure noise. By learning this reverse process, the model can generate new data samples from random noise.\n\n\n\nDiffusion models can be mathematically described using Stochastic Differential Equations (SDEs). J. N. Sohl-Dickstein et al. (2015) introduced the concept of diffusion probabilistic models, which can be viewed as a discrete-time approximation of continuous-time SDEs.\nA stochastic differential equation (SDE) in one-dimensional space can be expressed as: \\[dx = f(x, t)dt + g(t)dW_t \\quad dW_t \\sim \\mathcal{N}(0, dt)\\] where: - \\(x\\) is the state variable (e.g., pixel values of an image). - \\(t\\) is time. - \\(f(x, t)\\) is the drift coefficient, representing the deterministic part of the evolution of \\(x\\) over time. - \\(g(t)\\) is the diffusion coefficient, representing the stochastic part of the evolution. - \\(dW_t\\) is the increment of a Wiener process (or Brownian motion), which introduces randomness into the system.\nThe equation describes how the state variable \\(x\\) evolves over time under the influence of both deterministic and stochastic components. The drift term \\(f(x, t)dt\\) captures the systematic changes in \\(x\\), while the diffusion term \\(g(t)dW_t\\) captures the random fluctuations.\nIt is worth noting that when the deterministic component \\(f(x,t)\\) is negatively correlated with the position variable \\(x\\), the value of \\(x\\) will gradually move toward the origin as time progresses. Meanwhile, due to the presence of the stochastic term, \\(x\\) will eventually undergo random motion around the origin, following a Gaussian distribution.\n\n\n\nIn diffusion models, the forward diffusion process can be described by a specific type of SDE. The SDE is defined as: \\[dx = -\\frac{1}{2}\\beta(t)xdt + \\sqrt{\\beta(t)}dW_t\\quad dW_t \\sim \\mathcal{N}(0, dt)\\] where \\(\\beta(t)\\) is a time-dependent function that controls the rate of noise addition over time.\nSince \\(dW_t\\) follows a Gaussian distribution, we can discretize the SDE over small time intervals \\(\\Delta t\\) to obtain the following discrete-time update equation: \\[x_{t+\\Delta t} = x_t - \\frac{1}{2}\\beta(t)\\cdot x_t \\cdot dt + \\sqrt{\\beta(t)\\Delta t}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\\] where \\(\\epsilon_t\\) is a standard Gaussian noise term.\nThe following formula can be obtained from the Euler approximation and the Taylor expansion. \\[\nx_i = \\sqrt{1-\\beta_i}x_{i-1} + \\sqrt{\\beta_i}\\epsilon_{i-1}, \\quad \\epsilon_{i-1} \\sim \\mathcal{N}(0, I)\n\\tag{1}\\]\n\n\n\n\n\n\nNoteEuler-Maruyama and the Taylor expansion Method\n\n\n\n\n\npass\n\n\n\n\n\nLet \\(\\beta = 1 - \\alpha\\) in Equation 1. We can then derive the formula for \\(x_i\\) corresponding to a given \\(x_{i-1}\\). Since \\(\\alpha = [0.99999, \\dots, 0.999]\\), at each time step, \\(x_{t-1}\\) is scaled by \\(\\sqrt{\\alpha_t}\\) and some noise is added. Because the noise follows a Gaussian distribution, conditioned on \\(x_{t-1}\\), \\(x_t\\) also follows a Gaussian distribution \\(N\\). Then, a random sample from the distribution of \\(x_i\\) is used as the input for determining \\(x_{t+1}\\), and this process is iterated step by step. As a result, at each time step, \\(x_t\\) moves closer to the origin. When \\(T\\) is sufficiently large, i.e., there are enough time steps, the final \\(x_i\\) follows a standard normal distribution: \\(x_t \\sim N(0,1)\\). \\[\n\\left\\{\n\\begin{aligned}\n  x_t = \\sqrt{\\alpha_{t}} \\cdot x_{t-1} + \\sqrt{ 1- \\alpha_{t}} \\cdot \\epsilon_{t-1} \\\\\n  p(x_t|x_{t-1}) \\sim \\mathcal{N}(\\sqrt{\\alpha_{t}} \\cdot x_{t-1}, 1- \\alpha_{t})\n\\end{aligned}\n\\right.\n\\tag{2}\\] where \\(\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s\\).\n\n\n\n\nBy recursively applying the single-step diffusion process in Equation 2, we can derive the multi-step forward diffusion process. Specifically, we can express \\(x_t\\) directly in terms of the original data point \\(x_0\\) and the accumulated noise over \\(t\\) steps. This leads to the following equations: \\[\n\\left\\{\n\\begin{aligned}\n  x_t = \\sqrt{\\bar{\\alpha}_{t}} \\cdot x_0 + \\sqrt{ 1- \\bar{\\alpha}_{t}} \\cdot \\epsilon_t \\\\\n  p(x_t|x_0) \\sim \\mathcal{N}(\\sqrt{\\bar{\\alpha}_{t}} \\cdot x_0, 1- \\bar{\\alpha}_{t})\n\\end{aligned}\n\\right.\n\\tag{3}\\] where \\(\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s\\). \n\n\n\nThe reverse diffusion process aims to recover the original data point \\(x_0\\) from the noisy observation \\(x_t\\). Equation 1 gives us the single-step forward diffusion process, but only in the very first step, that is, when \\(x_0\\) is considered, \\(x_{t-1}\\) is a deterministic value. However, in subsequent steps, \\(x_{t-1}\\) is sampled from a probability distribution, and \\(\\epsilon\\) is also a stochastic value. The computation of \\(x_t\\) involves combining these two uncertain values through scaling and addition. A single \\(x_t\\) can correspond to infinitely many possible pairs of \\(x_{i-t}\\) and \\(\\epsilon_t\\). This means that the neural network would need to learn two uncertain values simultaneously, which is something the network cannot achieve.\nFor Equation 2, we can derive the reverse process as follows: \\[x_0 = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_{t}}\\epsilon_t}{\\sqrt{\\bar{\\alpha}_{t}}}\\] Since \\(\\sqrt{\\bar{\\alpha}_{t}}\\) is nearly zero, our network does not have sufficent precision.\nHowever, if we know the starting point \\(x_0\\) and the current point \\(x_t\\), the intermediate point \\(x_{t-1}\\) becomes a completely determined Gaussian distribution. Its closed-form expression can be derived as follows: \\[q(x_{t-1}|x_t, x_0) = \\mathcal{N}\\left(x_{t-1};  \\textcolor{red}{\\tilde{\\mu}_t(x_t, x_0)}, \\textcolor{blue}{\\tilde{\\beta}_t I}\\right)\\] where \\[\n\\begin{aligned}\n\\textcolor{red}{\\tilde{\\mu}_t(x_t, x_0)} &= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} x_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t} x_0 \\\\\n&= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} x_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t} (\\frac{x_t - \\sqrt{1 - \\bar{\\alpha}}\\epsilon_t}{\\sqrt{\\bar{\\alpha}_t}}) \\\\\n&= \\textcolor{red}{\\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_t \\right)} \\\\\n\\end{aligned}\n\\tag{4}\\] \\[\n\\textcolor{blue}{\\tilde{\\beta}_t} = \\textcolor{blue}{\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t}\n\\tag{5}\\]\n\n\n\n\n\n\nNoteThe derivation of \\(\\tilde{\\mu}_t\\) and \\(\\tilde{\\beta}_t\\)\n\n\n\n\n\nTo derive the expressions for \\(\\tilde{\\mu}_t\\) and \\(\\tilde{\\beta}_t\\), we can apply Bayes’ rule.\n\n\\[\\begin{aligned}\nq(x_{t-1}|x_t, x_0) &= q(x_t|x_{t-1})\\frac{q(x_{t-1}|x_0)}{q(x_{t}|x_0)} \\\\\n&\\propto \\exp \\Big(-\\frac{1}{2}(\\frac{\\left(x_t - \\sqrt{\\alpha_t}x_{t-1}\\right)^2}{\\beta_t} + \\frac{\\left(x_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}}x_0\\right)^2}{1-\\bar{\\alpha}_{t-1}} - \\frac{\\left(x_t - \\sqrt{\\alpha_t}x_0\\right)^2}{1 - \\bar{\\alpha}_t})\\Big) \\\\\n&= exp \\Big(-\\frac{1}{2} \\Big( (\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1-\\bar{\\alpha}_{t-1}})x_{t-1}^2 - 2(\\frac{\\sqrt{\\alpha_t}x_t}{\\beta_t} + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}x_0}{1-\\bar{\\alpha}_{t-1}})x_{t-1} + C(x_t, x_0) \\Big) \\Big) \\\\\n\n\\end{aligned}\\]\n\nAccording the standard form of Gaussian distribution, we can identify the mean and variance as follows:\n\n\\[\n\\textcolor{red}{\\tilde{\\mu}_t(x_t, x_0)} = (\\frac{\\sqrt{\\alpha_t}x_t}{\\beta_t} + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}x_0}{1-\\bar{\\alpha}_{t-1}})/(\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1-\\bar{\\alpha}_{t-1}})\\]\n\\[\n\\textcolor{blue}{\\tilde{\\beta}_t} = 1/(\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1-\\bar{\\alpha}_{t-1}}) = \\textcolor{blue}{\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t}\\]\n\nBased on the forward process formula \\(x_t = \\sqrt(\\bar{\\alpha}_{t})x_0 + \\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon_t\\), we can derive it as:\n\n\\[\nx_0 = \\frac{x_t-\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon}{\\sqrt{\\bar{\\alpha}_{t}}}\n\\]\n\nBy substituting this \\(x_0\\) into the formula for \\(\\tilde{\\mu}_t\\) and simplifying, we get\n\n\\[\n\\textcolor{red}{\\mu_t(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1-\\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon(x_t, t) \\right)}\n\\]\n\n\n\n\n\n\n\n\n\n\nThe forward diffusion process is essentially as Markov chain that gradually adds Gaussian noise to the data over a series of time steps until the data is completely transformed into pure random noise.\n\n\n\nGiven a data point \\(x_0 \\sim q(x)\\) from the data distribution, the forward diffusion process adds noise in \\(T\\) discrete time steps. At each time step \\(t\\), Gaussian noise is added to the data point according to a variance schedule \\(\\beta_t = (\\beta_1, \\beta_2, \\ldots, \\beta_T)\\). The noise addition at each step can be described by the following equation: \\[q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I)\\]\nwhere \\(\\beta_t\\) is the variance schedule at time step \\(t\\),which means at each time step, how much Gaussian noise is added to the data point and normally is chosen to be a small positive value. Meanswhile, \\(I\\) is the identity matrix, ensuring that the noise is isotropic (i.e., the same in all directions). Expected mean of \\(x_t\\) is scaled version of \\(x_{t-1}\\) by \\(\\sqrt{1-\\beta_t}\\), which ensures that as more noise is added, the original data point’s influence diminishes.\nIf we use the reparameterization trick, we can directly express the relation between \\(x_t\\) and \\(x_{t-1}\\) and further more between \\(x_t\\) and \\(x_0\\): \\[x_t = \\sqrt{1-\\beta_t}x_{t-1} + \\sqrt{\\beta_t}\\epsilon_{t-1}, \\quad \\epsilon_{t-1} \\sim \\mathcal{N}(0, I)\\] \\[x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\\] where \\(\\alpha_t = 1 - \\beta_t\\) and \\(\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s\\).\n\n\n\n\n\n\nNoteConversion from a distribution to an equation (The Reparameterization)\n\n\n\n\n\nIn the paper Ho, Jain, and Abbeel (2020), the authors define the forward diffusion process using a Gaussian distribution: \\[q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\underbrace{\\sqrt{1-\\beta_t}x_{t-1}}_{\\text{mean}\\,\\mu}, \\underbrace{\\beta_t I}_{\\text{variance}\\,\\sigma^2})\\] It means \\(x_t \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), and any variable that follows a Gaussian distribution can be expressed as: \\[ X = \\mu + \\sigma \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I) \\]\nIn our case, we have: \\[\n\\mu = \\sqrt{1-\\beta_t}x_{t-1}, \\quad \\sigma = \\sqrt{\\beta_t}\n\\] Thus, we can rewrite the forward diffusion step as: \\[\n\\begin{aligned}\nx_t = \\mu + \\sigma \\odot \\epsilon = \\sqrt{1-\\beta_t}x_{t-1} + \\sqrt{\\beta_t}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})\n\\end{aligned}\n\\]\nOtherwise, we can also express \\(x_t\\) directly in terms of \\(x_0\\): \\[\n\\begin{aligned}\nx_t &= \\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\\\\n&= \\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1}}x_{t-2} + \\sqrt{1-\\alpha_{t-1}}\\epsilon_{t-2}) + \\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\\\\n&= \\sqrt{\\alpha_t \\alpha_{t-1}}x_{t-2} + \\sqrt{\\alpha_t(1-\\alpha_{t-1})}\\epsilon_{t-2} + \\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\\\\n\\end{aligned}\n\\]\nSince \\(\\epsilon_{t-2}\\) and \\(\\epsilon_{t-1}\\) are both independent and identically distributed as \\(\\mathcal{N}(0, I)\\), according to the additivity property of Gaussian distributions: \\[\\mathcal{N}(\\mu_1, \\sigma_1^2) + \\mathcal{N}(\\mu_2, \\sigma_2^2) = \\mathcal{N}(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)\\] We can combine the noise terms: \\[\n\\sqrt{\\alpha_t(1-\\alpha_{t-1})}\\epsilon_{t-2} + \\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\sim \\mathcal{N}\\left(0, \\alpha_t(1-\\alpha_{t-1})I + (1-\\alpha_t)I\\right) = \\mathcal{N}\\left(0, (1 - \\alpha_t \\alpha_{t-1})I\\right)\n\\] Therefore: \\[\n\\begin{aligned}\nx_t &= \\sqrt{\\alpha_t \\alpha_{t-1}}x_{t-2} + \\sqrt{1 - \\alpha_t \\alpha_{t-1}}\\bar{\\epsilon}\\\\\n&=\\dots \\\\\n&= \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\mathbf{I}) \\\\\n\\end{aligned}\n\\] where \\(\\alpha_t = 1 - \\beta_t\\) and \\(\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s\\).\n\n\n\n\n\n\n\n\n\nNoteWhat is reparameterization and Why use it?\n\n\n\n\n\nIn deep generative models, we often need to sample from a probability distribution that depends on some parameters. For example, in diffusion models, a neural network predicts the mean \\(\\mu\\) and variance \\(\\sigma^2\\) of a Gaussian distribution at each time step, and then we need to sample a variable \\(x_t\\) from \\(\\mathcal{N}(\\mu, \\sigma^2)\\).\nHowever, sampling directly from this distribution can make it difficult to compute gradients with respect to the parameters during training. Suppose our neural network has parameters \\(\\theta\\), and we want to compute the gradient of some loss function \\(L\\) with respect to \\(\\theta\\). If we sample \\(x_t\\) directly from \\(\\mathcal{N}(\\mu_\\theta, \\sigma^2_\\theta)\\), the sampling operation is not a function, which introduces randomness that breaks the gradient flow, making it impossible to compute \\(\\nabla_\\theta L\\). The fatal problem is that the sampling operation is not differentiable.\nThe core idea of the reparameterization trick is to seperate the randomness from the parameters by expressing the random variable \\(x_t\\) as a deterministic function of the parameters and an independent noise variable. For a Gaussian distribution, we can express the sampling operation as:\\[x_t = \\mu_\\theta + \\sigma_\\theta \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 1)\\] Here, \\(\\epsilon \\sim \\mathcal{N}(0, 1)\\), which is a random variable drawn from a standard normal distribution and independent of the parameters \\(\\theta\\). This way, the randomness is isolated in \\(\\epsilon\\), and the rest of the expression is a deterministic function of \\(\\theta\\). It can be described by the following: \\[x_{t} = \\mu_{\\theta} + \\sigma_{\\theta} \\odot \\epsilon\\]\nNow, we can sample \\(x_t\\) by sampling \\(\\epsilon\\) from a fixed distribution (standard normal) and then applying the deterministic transformation. This allows us to compute gradients with respect to \\(\\theta\\) during backpropagation, as the sampling operation is now differentiable with respect to the parameters, which \\(\\nabla_\\mu x_{t} = 1\\) and \\(\\nabla_\\sigma x_{t} = \\epsilon\\).\nWe also can compare two methods below in Pytorch:\n\nimport torch\n\nmu = torch.tensor([1.0], requires_grad=True)\nsigma = torch.tensor([0.5], requires_grad=True)\n# Direct sampling (not differentiable)\ndistribution = torch.distributions.Normal(mu, sigma)\nx_direct = distribution.sample()  # sample() for non-differentiable case\n# x_direct = distribution.rsample()  # rsample() for reparameterized case\nloss_direct = (x_direct - 2.0) ** 2\n# loss_direct.backward()  # This would fail to compute gradients, if using rsample(), it works\n\n# Reparameterization trick (differentiable)\nepsilon = torch.randn(1)\nx_reparam = mu + sigma * epsilon  # reparameterized sample\nloss_reparam = (x_reparam - 2.0) ** 2\nloss_reparam.backward()  # This works\nprint(mu.grad)  # Gradient with respect to mu\nprint(sigma.grad)  # Gradient with respect to sigma\n\n\n\nWe can implement the forward process using PyTorch as follows:\n\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\ndef linear_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):\n    \"\"\"\n    Create a linear beta schedule.\n    \n    Parameters:\n    timesteps: int\n        The total number of time steps T.\n    beta_start: float\n        The starting value of beta.\n    beta_end: float\n        The ending value of beta.\n        \n    Returns:\n    beta_schedule: np.array\n        An array of beta values for each time step.\n    \"\"\"\n    return np.linspace(beta_start, beta_end, timesteps)\n\nT = 300  # Total diffusion steps\nbetas = linear_beta_schedule(T) # Linear beta schedule\nalphas = 1.0 - betas\nalpha_bars = np.cumprod(alphas, axis=0)  # Cumulative product of alphas\n\n\n\ndef forward_diffusion_sample(x_0, t, noise=None):\n    \"\"\"\n    Perform the forward diffusion process to obtain x_t from x_0.\n    \n    Parameters:\n    x_0: torch.Tensor\n        The original data point (batch_size, data_dim).\n    t: int\n        The time step at which to sample x_t.\n    noise: torch.Tensor or None\n        Optional noise to add. If None, random noise will be generated.\n        \n    Returns:\n    x_t: torch.Tensor\n        The noised data point at time step t.\n    \"\"\"\n    if noise is None:\n        noise = torch.randn_like(x_0)\n    \n    sqrt_alpha_bar_t = torch.sqrt(alpha_bars[t])[:, None, None, None]  \n    sqrt_one_minus_alpha_bar_t = torch.sqrt(1 - alpha_bars[t])[:, None, None, None] \n    \n    x_t = sqrt_alpha_bar_t * x_0 + sqrt_one_minus_alpha_bar_t * noise\n    return x_t\n\n\n\n\n\nIn the process of forward diffusion, we defined how to gradually add Gaussian noise to the data using \\(q(x_t|x_{t-1})\\). The reverse diffusion process aims to reverse this noising process, gradually removing the noise to recover the original data from pure noise by using its reverse conditional distribution \\(q(x_{t-1}|x_t)\\). If we can accurately model this reverse process, we can start from random noise \\(x_T \\sim \\mathcal{N}(0, I)\\) and iteratively sample \\(x_{T-1}, x_{T-2}, \\ldots, x_0\\) to generate new data samples that resemble the original data distribution.\n\n\nIn theory, if we have access to the true data distribution and the forward process, we can compute the reverse conditional distribution \\(q(x_{t-1}|x_t)\\) using Bayes’ theorem: \\[q(x_{t-1}|x_t) = \\frac{q(x_t|x_{t-1})q(x_{t-1})}{q(x_t)}\\] However, in practice, this is infeasible for several reasons:\n\nUnknown Data Distribution: We typically do not have access to the true data distribution \\(q(x - 1)\\). The reason is that we only have a finite dataset of samples from the data distribution, not the distribution itself.\nIntractable Integrals: The denominator \\(q(x_t)\\) is referred to as the marginal distribution of \\(x_t\\). If we want to compute the probability at a specific time step \\(t\\), we need to integrate over all possible previous states \\(x_{t-1}\\): \\[q(x_t) = \\int q(x_t|x_{t-1})q(x_{t-1})dx_{t-1}\\] This integral is often intractable, especially in high-dimensional spaces, which is called the “curse of dimensionality”.\n\nDue to these challenges, we cannot directly compute \\(q(x_{t-1}|x_t)\\) and instead need to approximate it using a parameterized model, such as a neural network.\n\n\n\nWe can use a neural network with parameters \\(\\theta\\) to approximate the reverse conditional distribution. When the diffusion step size \\(\\beta_t\\) is sufficiently small, the reverse process \\(q(x_{t-1} \\mid x_t)\\) also follows a Gaussian distribution. Therefore, we can define the neural network outputs as the parameters of this Gaussian distribution: \\[\np_\\theta(x_{0:T}) = p(x_T) \\prod_{t=1}^{T} p_\\theta(x_{t-1}|x_t) \\quad\np_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))\n\\] where \\(\\mu_\\theta(x_t, t)\\) and \\(\\Sigma_\\theta(x_t, t)\\) are the mean and covariance predicted by the neural network for the reverse step from \\(x_t\\) to \\(x_{t-1}\\).\nWhat is more, in the paper Ho, Jain, and Abbeel (2020), the authors find that using a fixed covariance \\(\\Sigma_\\theta(x_t, t) = \\beta_t I\\) works well in practice, simplifying the model to only predict the mean \\(\\mu_\\theta(x_t, t)\\), which is the main focus of the training process.\nNow, we can train the neural network to learn the parameters \\(\\theta\\) such that the reverse process \\(p_\\theta(x_{t-1}|x_t)\\) closely approximates the true but unknow reverse conditional distribution \\(q(x_{t-1}|x_t)\\). To train the model, we aim to maximize the log-likelihood of the real data \\(x_0\\) under the model, \\(\\log p_\\theta(x_0)\\). Because direct likelihood optimization is intractable, we resort to Jensen’s inequality and optimize the corresponding Evidence Lower Bound (ELBO): \\[- \\log p_\\theta(x_0) \\le \\mathbb{E}_{q} \\left[ \\log \\frac{q(x_{1:T}|x_0)}{p_\\theta(x_{0:T})} \\right] = \\text{ELBO}\\]\nTo convert each term in the equation to be analytically computable, the objective can be further rewritten to be a combination of several KL-divergence and entropy terms (See the detailed step-by-step process in J. Sohl-Dickstein et al. (2015)): \\[\\small \\text{ELBO} = \\mathbb{E}_{q} \\left[ \\underbrace{D_{KL}(q(x_T|x_0) || p_\\theta(x_T))}_{L_T}  + \\sum_{t=2}^{T} \\underbrace{D_{KL}(q(x_{t-1}|x_t, x_0) || p_\\theta(x_{t-1}|x_t))}_{L_{t-1}} \\underbrace{- \\log p_\\theta(x_0|x_1)}_{L_0} \\right]\\] where \\(D_{KL}(P||Q)\\) is the Kullback-Leibler divergence between two distributions \\(P\\) and \\(Q\\).\nAlthough the \\(q(x_{t-1}|x_t)\\) is intractable, \\(q(x_{t-1}|x_t, x_0)\\) is tractable because we condition on the original data point \\(x_0\\). While \\(x_0\\) is unknown at inference time, it is available during training as part of the dataset. Following the Equation 4 and Equation 5, we can derive the closed-form expression for \\(q(x_{t-1}|x_t, x_0)\\) as a Gaussian distribution with mean \\(\\tilde{\\mu}_t(x_t, x_0)\\) and variance \\(\\tilde{\\beta}_t I\\).\nNow, our current goal is to have the neural network \\(p_\\theta \\sim \\mathcal{N}\\left(\\mu_\\theta,\\Sigma_\\theta\\right)\\) approximate the true distribution \\(q(x_{t-1}|x_t, x_0) \\sim \\mathcal{N}\\left(\\tilde{\\mu}_t, \\tilde{\\beta}_t\\right)\\). Since the parameter \\(\\Sigma_\\theta\\) is often fixed as \\(\\beta_t I\\), we mainly focus on training the neural network to predict the mean \\(\\textcolor{red}{\\mu_\\theta}\\) to be as close as possible to the true mean \\(\\textcolor{red}{\\tilde{\\mu}_t = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_t \\right)}\\).\nThe equation means that to accurately predict the mean \\(\\mu_\\theta\\), the neural network only needs to predict the noise \\(\\epsilon_\\theta\\) present in the current image.\nThis can be achieved by minimizing the KL-divergence between these two distributions:\n\\[\n\\begin{aligned}\nL_{t} &= \\mathbb{E}_{x_0,\\epsilon} \\left[D_{KL}(q(x_{t-1}|x_t, x_0) || p_\\theta(x_{t-1}|x_t))\\right]\\\\\n&= \\mathbb{E}_{x_0,\\epsilon} \\left[\\frac{1}{2\\|\\Sigma_{\\theta}(x_t,t)\\|_2 ^ 2}\\| \\tilde{\\mu}_t(x_t, x_0) - \\mu_\\theta(x_t, t) \\|^2\\right]\\\\\n&= \\mathbb{E}_{x_0,\\epsilon} \\left[\\frac{1}{2\\|\\Sigma_{\\theta}(x_t,t)\\|_2 ^ 2}\\| \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_t \\right) - \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right) \\|^{2}\\right]\\\\\n&= \\mathbb{E}_{x_0,\\epsilon} \\left[\\frac{(1-\\alpha_t)^2}{2\\alpha_t(1-\\bar{\\alpha}_t)\\| \\Sigma_\\theta \\|_2^2}\n\\left\\lVert\n\\epsilon_t - \\epsilon_\\theta(x_t,t)\n\\right\\rVert^2\n\\right] \\\\\n&= \\mathbb{E}_{x_0,\\epsilon}\n\\left[\n\\frac{(1-\\alpha_t)^2}\n{2\\alpha_t(1-\\bar{\\alpha}_t)\\left\\lVert \\Sigma_\\theta \\right\\rVert _2^2}\n\\left\\lVert\n\\epsilon_t -\n\\epsilon_\\theta\\!\\left(\n\\sqrt{\\bar{\\alpha}_t} x_0 +\n\\sqrt{1-\\bar{\\alpha}_t}\\,\\epsilon_t,\nt\n\\right)\n\\right\\rVert^2\n\\right]\n\\end{aligned}\n\\]\nHo, Jain, and Abbeel (2020) further simplifies the loss function by removing the weighting term, leading to a more straightforward objective:\n\\[\n\\boxed {\\textcolor{red}{L_t^{simple} = \\mathbb{E}_{x_0,\\epsilon,t} \\left[\\left\\lVert \\epsilon_t - \\epsilon_\\theta(x_t,t) \\right\\rVert^2\\right]}}\n\\]\nThe final training objective is:\n\\[\nL_{simple} = L_t^{simple} + C\n\\]\nwhere \\(C\\) is a constant that does not depend on the model parameters \\(\\theta\\) and can be ignored during optimization.\n\n\n\n\nOnce the neural network which can predict the noise \\(\\epsilon_\\theta(x_t, t)\\) is trained, we can use it to sample new data points by reversing the diffusion process. Starting from pure Gaussian noise \\(x_T \\sim \\mathcal{N}(0, I)\\), we iteratively apply the reverse diffusion steps using the learned model.\n\\[\nx_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right) + \\sigma_t z\n\\]\nwhere \\(z \\sim \\mathcal{N}(0, I)\\) is random noise added at each step to maintain stochasticity, and \\(\\sigma_t\\) is a scaling factor that can be adjusted based on the desired level of noise during sampling.\n\n\n\n\n\n\nNoteWhy we need to add \\(\\sigma_t z\\)？\n\n\n\nThis term corresponds to Langevin dynamics. Without it, the sampling process becomes a deterministic numerical procedure, which often leads to overly smooth images with a lack of fine details. Introducing stochastic perturbations helps correct estimation errors at each step and enables the generative model to better explore the true image manifold.\n\n\n\n\n\nWe get the simplified loss function as follows:\n\\[\nL_{simple} = \\left\\|\\epsilon_t - \\epsilon_\\theta(x_t,t) \\right\\|^2\n\\]\nThis means that the training procedure consists of only four steps:\n\nRandom sampling: Randomly sample a data point \\(x_0\\) from the training dataset.\nRandom timestep: Select a random time step $ t $.\nAdd noise in the forward process: Generate a noisy version of the data point \\(x_t\\) using the forward diffusion equation:\n\\[ x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0, I)\\]\nTrain the model: Use the noisy image \\(x_t\\) and the corresponding noise \\(\\epsilon_t\\) to train the neural network \\(\\epsilon_\\theta(x_t, t)\\) to predict the noise.\nLoss computation: Compute the loss using the simplified loss function:\n\\[ L_{simple} = \\left\\|\\epsilon_t - \\epsilon_\\theta(x_t,t) \\right\\|^2 \\]"
  },
  {
    "objectID": "posts/diffusion-model/index.html#what-is-diffusion-model",
    "href": "posts/diffusion-model/index.html#what-is-diffusion-model",
    "title": "Diffusion Model",
    "section": "",
    "text": "Diffusion models are a class of image generative neural networks inspired by and formulated as a simulation of the thermodynamic diffusion process. The core idea is to model the data generation process as a gradual denoising process, where a neural network learns to reverse a diffusion process that progressively adds noise to the data until it becomes pure noise. By learning this reverse process, the model can generate new data samples from random noise."
  },
  {
    "objectID": "posts/diffusion-model/index.html#sde-stochastic-differential-equations",
    "href": "posts/diffusion-model/index.html#sde-stochastic-differential-equations",
    "title": "Diffusion Model",
    "section": "",
    "text": "Diffusion models can be mathematically described using Stochastic Differential Equations (SDEs). J. N. Sohl-Dickstein et al. (2015) introduced the concept of diffusion probabilistic models, which can be viewed as a discrete-time approximation of continuous-time SDEs.\nA stochastic differential equation (SDE) in one-dimensional space can be expressed as: \\[dx = f(x, t)dt + g(t)dW_t \\quad dW_t \\sim \\mathcal{N}(0, dt)\\] where: - \\(x\\) is the state variable (e.g., pixel values of an image). - \\(t\\) is time. - \\(f(x, t)\\) is the drift coefficient, representing the deterministic part of the evolution of \\(x\\) over time. - \\(g(t)\\) is the diffusion coefficient, representing the stochastic part of the evolution. - \\(dW_t\\) is the increment of a Wiener process (or Brownian motion), which introduces randomness into the system.\nThe equation describes how the state variable \\(x\\) evolves over time under the influence of both deterministic and stochastic components. The drift term \\(f(x, t)dt\\) captures the systematic changes in \\(x\\), while the diffusion term \\(g(t)dW_t\\) captures the random fluctuations.\nIt is worth noting that when the deterministic component \\(f(x,t)\\) is negatively correlated with the position variable \\(x\\), the value of \\(x\\) will gradually move toward the origin as time progresses. Meanwhile, due to the presence of the stochastic term, \\(x\\) will eventually undergo random motion around the origin, following a Gaussian distribution."
  },
  {
    "objectID": "posts/diffusion-model/index.html#from-sde-to-diffusion-models",
    "href": "posts/diffusion-model/index.html#from-sde-to-diffusion-models",
    "title": "Diffusion Model",
    "section": "",
    "text": "In diffusion models, the forward diffusion process can be described by a specific type of SDE. The SDE is defined as: \\[dx = -\\frac{1}{2}\\beta(t)xdt + \\sqrt{\\beta(t)}dW_t\\quad dW_t \\sim \\mathcal{N}(0, dt)\\] where \\(\\beta(t)\\) is a time-dependent function that controls the rate of noise addition over time.\nSince \\(dW_t\\) follows a Gaussian distribution, we can discretize the SDE over small time intervals \\(\\Delta t\\) to obtain the following discrete-time update equation: \\[x_{t+\\Delta t} = x_t - \\frac{1}{2}\\beta(t)\\cdot x_t \\cdot dt + \\sqrt{\\beta(t)\\Delta t}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\\] where \\(\\epsilon_t\\) is a standard Gaussian noise term.\nThe following formula can be obtained from the Euler approximation and the Taylor expansion. \\[\nx_i = \\sqrt{1-\\beta_i}x_{i-1} + \\sqrt{\\beta_i}\\epsilon_{i-1}, \\quad \\epsilon_{i-1} \\sim \\mathcal{N}(0, I)\n\\tag{1}\\]\n\n\n\n\n\n\nNoteEuler-Maruyama and the Taylor expansion Method\n\n\n\n\n\npass\n\n\n\n\n\nLet \\(\\beta = 1 - \\alpha\\) in Equation 1. We can then derive the formula for \\(x_i\\) corresponding to a given \\(x_{i-1}\\). Since \\(\\alpha = [0.99999, \\dots, 0.999]\\), at each time step, \\(x_{t-1}\\) is scaled by \\(\\sqrt{\\alpha_t}\\) and some noise is added. Because the noise follows a Gaussian distribution, conditioned on \\(x_{t-1}\\), \\(x_t\\) also follows a Gaussian distribution \\(N\\). Then, a random sample from the distribution of \\(x_i\\) is used as the input for determining \\(x_{t+1}\\), and this process is iterated step by step. As a result, at each time step, \\(x_t\\) moves closer to the origin. When \\(T\\) is sufficiently large, i.e., there are enough time steps, the final \\(x_i\\) follows a standard normal distribution: \\(x_t \\sim N(0,1)\\). \\[\n\\left\\{\n\\begin{aligned}\n  x_t = \\sqrt{\\alpha_{t}} \\cdot x_{t-1} + \\sqrt{ 1- \\alpha_{t}} \\cdot \\epsilon_{t-1} \\\\\n  p(x_t|x_{t-1}) \\sim \\mathcal{N}(\\sqrt{\\alpha_{t}} \\cdot x_{t-1}, 1- \\alpha_{t})\n\\end{aligned}\n\\right.\n\\tag{2}\\] where \\(\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s\\).\n\n\n\n\nBy recursively applying the single-step diffusion process in Equation 2, we can derive the multi-step forward diffusion process. Specifically, we can express \\(x_t\\) directly in terms of the original data point \\(x_0\\) and the accumulated noise over \\(t\\) steps. This leads to the following equations: \\[\n\\left\\{\n\\begin{aligned}\n  x_t = \\sqrt{\\bar{\\alpha}_{t}} \\cdot x_0 + \\sqrt{ 1- \\bar{\\alpha}_{t}} \\cdot \\epsilon_t \\\\\n  p(x_t|x_0) \\sim \\mathcal{N}(\\sqrt{\\bar{\\alpha}_{t}} \\cdot x_0, 1- \\bar{\\alpha}_{t})\n\\end{aligned}\n\\right.\n\\tag{3}\\] where \\(\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s\\). \n\n\n\nThe reverse diffusion process aims to recover the original data point \\(x_0\\) from the noisy observation \\(x_t\\). Equation 1 gives us the single-step forward diffusion process, but only in the very first step, that is, when \\(x_0\\) is considered, \\(x_{t-1}\\) is a deterministic value. However, in subsequent steps, \\(x_{t-1}\\) is sampled from a probability distribution, and \\(\\epsilon\\) is also a stochastic value. The computation of \\(x_t\\) involves combining these two uncertain values through scaling and addition. A single \\(x_t\\) can correspond to infinitely many possible pairs of \\(x_{i-t}\\) and \\(\\epsilon_t\\). This means that the neural network would need to learn two uncertain values simultaneously, which is something the network cannot achieve.\nFor Equation 2, we can derive the reverse process as follows: \\[x_0 = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_{t}}\\epsilon_t}{\\sqrt{\\bar{\\alpha}_{t}}}\\] Since \\(\\sqrt{\\bar{\\alpha}_{t}}\\) is nearly zero, our network does not have sufficent precision.\nHowever, if we know the starting point \\(x_0\\) and the current point \\(x_t\\), the intermediate point \\(x_{t-1}\\) becomes a completely determined Gaussian distribution. Its closed-form expression can be derived as follows: \\[q(x_{t-1}|x_t, x_0) = \\mathcal{N}\\left(x_{t-1};  \\textcolor{red}{\\tilde{\\mu}_t(x_t, x_0)}, \\textcolor{blue}{\\tilde{\\beta}_t I}\\right)\\] where \\[\n\\begin{aligned}\n\\textcolor{red}{\\tilde{\\mu}_t(x_t, x_0)} &= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} x_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t} x_0 \\\\\n&= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} x_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t} (\\frac{x_t - \\sqrt{1 - \\bar{\\alpha}}\\epsilon_t}{\\sqrt{\\bar{\\alpha}_t}}) \\\\\n&= \\textcolor{red}{\\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_t \\right)} \\\\\n\\end{aligned}\n\\tag{4}\\] \\[\n\\textcolor{blue}{\\tilde{\\beta}_t} = \\textcolor{blue}{\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t}\n\\tag{5}\\]\n\n\n\n\n\n\nNoteThe derivation of \\(\\tilde{\\mu}_t\\) and \\(\\tilde{\\beta}_t\\)\n\n\n\n\n\nTo derive the expressions for \\(\\tilde{\\mu}_t\\) and \\(\\tilde{\\beta}_t\\), we can apply Bayes’ rule.\n\n\\[\\begin{aligned}\nq(x_{t-1}|x_t, x_0) &= q(x_t|x_{t-1})\\frac{q(x_{t-1}|x_0)}{q(x_{t}|x_0)} \\\\\n&\\propto \\exp \\Big(-\\frac{1}{2}(\\frac{\\left(x_t - \\sqrt{\\alpha_t}x_{t-1}\\right)^2}{\\beta_t} + \\frac{\\left(x_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}}x_0\\right)^2}{1-\\bar{\\alpha}_{t-1}} - \\frac{\\left(x_t - \\sqrt{\\alpha_t}x_0\\right)^2}{1 - \\bar{\\alpha}_t})\\Big) \\\\\n&= exp \\Big(-\\frac{1}{2} \\Big( (\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1-\\bar{\\alpha}_{t-1}})x_{t-1}^2 - 2(\\frac{\\sqrt{\\alpha_t}x_t}{\\beta_t} + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}x_0}{1-\\bar{\\alpha}_{t-1}})x_{t-1} + C(x_t, x_0) \\Big) \\Big) \\\\\n\n\\end{aligned}\\]\n\nAccording the standard form of Gaussian distribution, we can identify the mean and variance as follows:\n\n\\[\n\\textcolor{red}{\\tilde{\\mu}_t(x_t, x_0)} = (\\frac{\\sqrt{\\alpha_t}x_t}{\\beta_t} + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}x_0}{1-\\bar{\\alpha}_{t-1}})/(\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1-\\bar{\\alpha}_{t-1}})\\]\n\\[\n\\textcolor{blue}{\\tilde{\\beta}_t} = 1/(\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1-\\bar{\\alpha}_{t-1}}) = \\textcolor{blue}{\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t}\\]\n\nBased on the forward process formula \\(x_t = \\sqrt(\\bar{\\alpha}_{t})x_0 + \\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon_t\\), we can derive it as:\n\n\\[\nx_0 = \\frac{x_t-\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon}{\\sqrt{\\bar{\\alpha}_{t}}}\n\\]\n\nBy substituting this \\(x_0\\) into the formula for \\(\\tilde{\\mu}_t\\) and simplifying, we get\n\n\\[\n\\textcolor{red}{\\mu_t(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1-\\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon(x_t, t) \\right)}\n\\]"
  },
  {
    "objectID": "posts/diffusion-model/index.html#ddpm-denoising-diffusion-probabilistic-models",
    "href": "posts/diffusion-model/index.html#ddpm-denoising-diffusion-probabilistic-models",
    "title": "Diffusion Model",
    "section": "",
    "text": "The forward diffusion process is essentially as Markov chain that gradually adds Gaussian noise to the data over a series of time steps until the data is completely transformed into pure random noise.\n\n\n\nGiven a data point \\(x_0 \\sim q(x)\\) from the data distribution, the forward diffusion process adds noise in \\(T\\) discrete time steps. At each time step \\(t\\), Gaussian noise is added to the data point according to a variance schedule \\(\\beta_t = (\\beta_1, \\beta_2, \\ldots, \\beta_T)\\). The noise addition at each step can be described by the following equation: \\[q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I)\\]\nwhere \\(\\beta_t\\) is the variance schedule at time step \\(t\\),which means at each time step, how much Gaussian noise is added to the data point and normally is chosen to be a small positive value. Meanswhile, \\(I\\) is the identity matrix, ensuring that the noise is isotropic (i.e., the same in all directions). Expected mean of \\(x_t\\) is scaled version of \\(x_{t-1}\\) by \\(\\sqrt{1-\\beta_t}\\), which ensures that as more noise is added, the original data point’s influence diminishes.\nIf we use the reparameterization trick, we can directly express the relation between \\(x_t\\) and \\(x_{t-1}\\) and further more between \\(x_t\\) and \\(x_0\\): \\[x_t = \\sqrt{1-\\beta_t}x_{t-1} + \\sqrt{\\beta_t}\\epsilon_{t-1}, \\quad \\epsilon_{t-1} \\sim \\mathcal{N}(0, I)\\] \\[x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\\] where \\(\\alpha_t = 1 - \\beta_t\\) and \\(\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s\\).\n\n\n\n\n\n\nNoteConversion from a distribution to an equation (The Reparameterization)\n\n\n\n\n\nIn the paper Ho, Jain, and Abbeel (2020), the authors define the forward diffusion process using a Gaussian distribution: \\[q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\underbrace{\\sqrt{1-\\beta_t}x_{t-1}}_{\\text{mean}\\,\\mu}, \\underbrace{\\beta_t I}_{\\text{variance}\\,\\sigma^2})\\] It means \\(x_t \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), and any variable that follows a Gaussian distribution can be expressed as: \\[ X = \\mu + \\sigma \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I) \\]\nIn our case, we have: \\[\n\\mu = \\sqrt{1-\\beta_t}x_{t-1}, \\quad \\sigma = \\sqrt{\\beta_t}\n\\] Thus, we can rewrite the forward diffusion step as: \\[\n\\begin{aligned}\nx_t = \\mu + \\sigma \\odot \\epsilon = \\sqrt{1-\\beta_t}x_{t-1} + \\sqrt{\\beta_t}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})\n\\end{aligned}\n\\]\nOtherwise, we can also express \\(x_t\\) directly in terms of \\(x_0\\): \\[\n\\begin{aligned}\nx_t &= \\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\\\\n&= \\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1}}x_{t-2} + \\sqrt{1-\\alpha_{t-1}}\\epsilon_{t-2}) + \\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\\\\n&= \\sqrt{\\alpha_t \\alpha_{t-1}}x_{t-2} + \\sqrt{\\alpha_t(1-\\alpha_{t-1})}\\epsilon_{t-2} + \\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\\\\n\\end{aligned}\n\\]\nSince \\(\\epsilon_{t-2}\\) and \\(\\epsilon_{t-1}\\) are both independent and identically distributed as \\(\\mathcal{N}(0, I)\\), according to the additivity property of Gaussian distributions: \\[\\mathcal{N}(\\mu_1, \\sigma_1^2) + \\mathcal{N}(\\mu_2, \\sigma_2^2) = \\mathcal{N}(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)\\] We can combine the noise terms: \\[\n\\sqrt{\\alpha_t(1-\\alpha_{t-1})}\\epsilon_{t-2} + \\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\sim \\mathcal{N}\\left(0, \\alpha_t(1-\\alpha_{t-1})I + (1-\\alpha_t)I\\right) = \\mathcal{N}\\left(0, (1 - \\alpha_t \\alpha_{t-1})I\\right)\n\\] Therefore: \\[\n\\begin{aligned}\nx_t &= \\sqrt{\\alpha_t \\alpha_{t-1}}x_{t-2} + \\sqrt{1 - \\alpha_t \\alpha_{t-1}}\\bar{\\epsilon}\\\\\n&=\\dots \\\\\n&= \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\mathbf{I}) \\\\\n\\end{aligned}\n\\] where \\(\\alpha_t = 1 - \\beta_t\\) and \\(\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s\\).\n\n\n\n\n\n\n\n\n\nNoteWhat is reparameterization and Why use it?\n\n\n\n\n\nIn deep generative models, we often need to sample from a probability distribution that depends on some parameters. For example, in diffusion models, a neural network predicts the mean \\(\\mu\\) and variance \\(\\sigma^2\\) of a Gaussian distribution at each time step, and then we need to sample a variable \\(x_t\\) from \\(\\mathcal{N}(\\mu, \\sigma^2)\\).\nHowever, sampling directly from this distribution can make it difficult to compute gradients with respect to the parameters during training. Suppose our neural network has parameters \\(\\theta\\), and we want to compute the gradient of some loss function \\(L\\) with respect to \\(\\theta\\). If we sample \\(x_t\\) directly from \\(\\mathcal{N}(\\mu_\\theta, \\sigma^2_\\theta)\\), the sampling operation is not a function, which introduces randomness that breaks the gradient flow, making it impossible to compute \\(\\nabla_\\theta L\\). The fatal problem is that the sampling operation is not differentiable.\nThe core idea of the reparameterization trick is to seperate the randomness from the parameters by expressing the random variable \\(x_t\\) as a deterministic function of the parameters and an independent noise variable. For a Gaussian distribution, we can express the sampling operation as:\\[x_t = \\mu_\\theta + \\sigma_\\theta \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 1)\\] Here, \\(\\epsilon \\sim \\mathcal{N}(0, 1)\\), which is a random variable drawn from a standard normal distribution and independent of the parameters \\(\\theta\\). This way, the randomness is isolated in \\(\\epsilon\\), and the rest of the expression is a deterministic function of \\(\\theta\\). It can be described by the following: \\[x_{t} = \\mu_{\\theta} + \\sigma_{\\theta} \\odot \\epsilon\\]\nNow, we can sample \\(x_t\\) by sampling \\(\\epsilon\\) from a fixed distribution (standard normal) and then applying the deterministic transformation. This allows us to compute gradients with respect to \\(\\theta\\) during backpropagation, as the sampling operation is now differentiable with respect to the parameters, which \\(\\nabla_\\mu x_{t} = 1\\) and \\(\\nabla_\\sigma x_{t} = \\epsilon\\).\nWe also can compare two methods below in Pytorch:\n\nimport torch\n\nmu = torch.tensor([1.0], requires_grad=True)\nsigma = torch.tensor([0.5], requires_grad=True)\n# Direct sampling (not differentiable)\ndistribution = torch.distributions.Normal(mu, sigma)\nx_direct = distribution.sample()  # sample() for non-differentiable case\n# x_direct = distribution.rsample()  # rsample() for reparameterized case\nloss_direct = (x_direct - 2.0) ** 2\n# loss_direct.backward()  # This would fail to compute gradients, if using rsample(), it works\n\n# Reparameterization trick (differentiable)\nepsilon = torch.randn(1)\nx_reparam = mu + sigma * epsilon  # reparameterized sample\nloss_reparam = (x_reparam - 2.0) ** 2\nloss_reparam.backward()  # This works\nprint(mu.grad)  # Gradient with respect to mu\nprint(sigma.grad)  # Gradient with respect to sigma\n\n\n\nWe can implement the forward process using PyTorch as follows:\n\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\ndef linear_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):\n    \"\"\"\n    Create a linear beta schedule.\n    \n    Parameters:\n    timesteps: int\n        The total number of time steps T.\n    beta_start: float\n        The starting value of beta.\n    beta_end: float\n        The ending value of beta.\n        \n    Returns:\n    beta_schedule: np.array\n        An array of beta values for each time step.\n    \"\"\"\n    return np.linspace(beta_start, beta_end, timesteps)\n\nT = 300  # Total diffusion steps\nbetas = linear_beta_schedule(T) # Linear beta schedule\nalphas = 1.0 - betas\nalpha_bars = np.cumprod(alphas, axis=0)  # Cumulative product of alphas\n\n\n\ndef forward_diffusion_sample(x_0, t, noise=None):\n    \"\"\"\n    Perform the forward diffusion process to obtain x_t from x_0.\n    \n    Parameters:\n    x_0: torch.Tensor\n        The original data point (batch_size, data_dim).\n    t: int\n        The time step at which to sample x_t.\n    noise: torch.Tensor or None\n        Optional noise to add. If None, random noise will be generated.\n        \n    Returns:\n    x_t: torch.Tensor\n        The noised data point at time step t.\n    \"\"\"\n    if noise is None:\n        noise = torch.randn_like(x_0)\n    \n    sqrt_alpha_bar_t = torch.sqrt(alpha_bars[t])[:, None, None, None]  \n    sqrt_one_minus_alpha_bar_t = torch.sqrt(1 - alpha_bars[t])[:, None, None, None] \n    \n    x_t = sqrt_alpha_bar_t * x_0 + sqrt_one_minus_alpha_bar_t * noise\n    return x_t"
  },
  {
    "objectID": "posts/diffusion-model/index.html#reverse-diffusion-process",
    "href": "posts/diffusion-model/index.html#reverse-diffusion-process",
    "title": "Diffusion Model",
    "section": "",
    "text": "In the process of forward diffusion, we defined how to gradually add Gaussian noise to the data using \\(q(x_t|x_{t-1})\\). The reverse diffusion process aims to reverse this noising process, gradually removing the noise to recover the original data from pure noise by using its reverse conditional distribution \\(q(x_{t-1}|x_t)\\). If we can accurately model this reverse process, we can start from random noise \\(x_T \\sim \\mathcal{N}(0, I)\\) and iteratively sample \\(x_{T-1}, x_{T-2}, \\ldots, x_0\\) to generate new data samples that resemble the original data distribution.\n\n\nIn theory, if we have access to the true data distribution and the forward process, we can compute the reverse conditional distribution \\(q(x_{t-1}|x_t)\\) using Bayes’ theorem: \\[q(x_{t-1}|x_t) = \\frac{q(x_t|x_{t-1})q(x_{t-1})}{q(x_t)}\\] However, in practice, this is infeasible for several reasons:\n\nUnknown Data Distribution: We typically do not have access to the true data distribution \\(q(x - 1)\\). The reason is that we only have a finite dataset of samples from the data distribution, not the distribution itself.\nIntractable Integrals: The denominator \\(q(x_t)\\) is referred to as the marginal distribution of \\(x_t\\). If we want to compute the probability at a specific time step \\(t\\), we need to integrate over all possible previous states \\(x_{t-1}\\): \\[q(x_t) = \\int q(x_t|x_{t-1})q(x_{t-1})dx_{t-1}\\] This integral is often intractable, especially in high-dimensional spaces, which is called the “curse of dimensionality”.\n\nDue to these challenges, we cannot directly compute \\(q(x_{t-1}|x_t)\\) and instead need to approximate it using a parameterized model, such as a neural network.\n\n\n\nWe can use a neural network with parameters \\(\\theta\\) to approximate the reverse conditional distribution. When the diffusion step size \\(\\beta_t\\) is sufficiently small, the reverse process \\(q(x_{t-1} \\mid x_t)\\) also follows a Gaussian distribution. Therefore, we can define the neural network outputs as the parameters of this Gaussian distribution: \\[\np_\\theta(x_{0:T}) = p(x_T) \\prod_{t=1}^{T} p_\\theta(x_{t-1}|x_t) \\quad\np_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))\n\\] where \\(\\mu_\\theta(x_t, t)\\) and \\(\\Sigma_\\theta(x_t, t)\\) are the mean and covariance predicted by the neural network for the reverse step from \\(x_t\\) to \\(x_{t-1}\\).\nWhat is more, in the paper Ho, Jain, and Abbeel (2020), the authors find that using a fixed covariance \\(\\Sigma_\\theta(x_t, t) = \\beta_t I\\) works well in practice, simplifying the model to only predict the mean \\(\\mu_\\theta(x_t, t)\\), which is the main focus of the training process.\nNow, we can train the neural network to learn the parameters \\(\\theta\\) such that the reverse process \\(p_\\theta(x_{t-1}|x_t)\\) closely approximates the true but unknow reverse conditional distribution \\(q(x_{t-1}|x_t)\\). To train the model, we aim to maximize the log-likelihood of the real data \\(x_0\\) under the model, \\(\\log p_\\theta(x_0)\\). Because direct likelihood optimization is intractable, we resort to Jensen’s inequality and optimize the corresponding Evidence Lower Bound (ELBO): \\[- \\log p_\\theta(x_0) \\le \\mathbb{E}_{q} \\left[ \\log \\frac{q(x_{1:T}|x_0)}{p_\\theta(x_{0:T})} \\right] = \\text{ELBO}\\]\nTo convert each term in the equation to be analytically computable, the objective can be further rewritten to be a combination of several KL-divergence and entropy terms (See the detailed step-by-step process in J. Sohl-Dickstein et al. (2015)): \\[\\small \\text{ELBO} = \\mathbb{E}_{q} \\left[ \\underbrace{D_{KL}(q(x_T|x_0) || p_\\theta(x_T))}_{L_T}  + \\sum_{t=2}^{T} \\underbrace{D_{KL}(q(x_{t-1}|x_t, x_0) || p_\\theta(x_{t-1}|x_t))}_{L_{t-1}} \\underbrace{- \\log p_\\theta(x_0|x_1)}_{L_0} \\right]\\] where \\(D_{KL}(P||Q)\\) is the Kullback-Leibler divergence between two distributions \\(P\\) and \\(Q\\).\nAlthough the \\(q(x_{t-1}|x_t)\\) is intractable, \\(q(x_{t-1}|x_t, x_0)\\) is tractable because we condition on the original data point \\(x_0\\). While \\(x_0\\) is unknown at inference time, it is available during training as part of the dataset. Following the Equation 4 and Equation 5, we can derive the closed-form expression for \\(q(x_{t-1}|x_t, x_0)\\) as a Gaussian distribution with mean \\(\\tilde{\\mu}_t(x_t, x_0)\\) and variance \\(\\tilde{\\beta}_t I\\).\nNow, our current goal is to have the neural network \\(p_\\theta \\sim \\mathcal{N}\\left(\\mu_\\theta,\\Sigma_\\theta\\right)\\) approximate the true distribution \\(q(x_{t-1}|x_t, x_0) \\sim \\mathcal{N}\\left(\\tilde{\\mu}_t, \\tilde{\\beta}_t\\right)\\). Since the parameter \\(\\Sigma_\\theta\\) is often fixed as \\(\\beta_t I\\), we mainly focus on training the neural network to predict the mean \\(\\textcolor{red}{\\mu_\\theta}\\) to be as close as possible to the true mean \\(\\textcolor{red}{\\tilde{\\mu}_t = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_t \\right)}\\).\nThe equation means that to accurately predict the mean \\(\\mu_\\theta\\), the neural network only needs to predict the noise \\(\\epsilon_\\theta\\) present in the current image.\nThis can be achieved by minimizing the KL-divergence between these two distributions:\n\\[\n\\begin{aligned}\nL_{t} &= \\mathbb{E}_{x_0,\\epsilon} \\left[D_{KL}(q(x_{t-1}|x_t, x_0) || p_\\theta(x_{t-1}|x_t))\\right]\\\\\n&= \\mathbb{E}_{x_0,\\epsilon} \\left[\\frac{1}{2\\|\\Sigma_{\\theta}(x_t,t)\\|_2 ^ 2}\\| \\tilde{\\mu}_t(x_t, x_0) - \\mu_\\theta(x_t, t) \\|^2\\right]\\\\\n&= \\mathbb{E}_{x_0,\\epsilon} \\left[\\frac{1}{2\\|\\Sigma_{\\theta}(x_t,t)\\|_2 ^ 2}\\| \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_t \\right) - \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right) \\|^{2}\\right]\\\\\n&= \\mathbb{E}_{x_0,\\epsilon} \\left[\\frac{(1-\\alpha_t)^2}{2\\alpha_t(1-\\bar{\\alpha}_t)\\| \\Sigma_\\theta \\|_2^2}\n\\left\\lVert\n\\epsilon_t - \\epsilon_\\theta(x_t,t)\n\\right\\rVert^2\n\\right] \\\\\n&= \\mathbb{E}_{x_0,\\epsilon}\n\\left[\n\\frac{(1-\\alpha_t)^2}\n{2\\alpha_t(1-\\bar{\\alpha}_t)\\left\\lVert \\Sigma_\\theta \\right\\rVert _2^2}\n\\left\\lVert\n\\epsilon_t -\n\\epsilon_\\theta\\!\\left(\n\\sqrt{\\bar{\\alpha}_t} x_0 +\n\\sqrt{1-\\bar{\\alpha}_t}\\,\\epsilon_t,\nt\n\\right)\n\\right\\rVert^2\n\\right]\n\\end{aligned}\n\\]\nHo, Jain, and Abbeel (2020) further simplifies the loss function by removing the weighting term, leading to a more straightforward objective:\n\\[\n\\boxed {\\textcolor{red}{L_t^{simple} = \\mathbb{E}_{x_0,\\epsilon,t} \\left[\\left\\lVert \\epsilon_t - \\epsilon_\\theta(x_t,t) \\right\\rVert^2\\right]}}\n\\]\nThe final training objective is:\n\\[\nL_{simple} = L_t^{simple} + C\n\\]\nwhere \\(C\\) is a constant that does not depend on the model parameters \\(\\theta\\) and can be ignored during optimization.\n\n\n\n\nOnce the neural network which can predict the noise \\(\\epsilon_\\theta(x_t, t)\\) is trained, we can use it to sample new data points by reversing the diffusion process. Starting from pure Gaussian noise \\(x_T \\sim \\mathcal{N}(0, I)\\), we iteratively apply the reverse diffusion steps using the learned model.\n\\[\nx_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right) + \\sigma_t z\n\\]\nwhere \\(z \\sim \\mathcal{N}(0, I)\\) is random noise added at each step to maintain stochasticity, and \\(\\sigma_t\\) is a scaling factor that can be adjusted based on the desired level of noise during sampling.\n\n\n\n\n\n\nNoteWhy we need to add \\(\\sigma_t z\\)？\n\n\n\nThis term corresponds to Langevin dynamics. Without it, the sampling process becomes a deterministic numerical procedure, which often leads to overly smooth images with a lack of fine details. Introducing stochastic perturbations helps correct estimation errors at each step and enables the generative model to better explore the true image manifold.\n\n\n\n\n\nWe get the simplified loss function as follows:\n\\[\nL_{simple} = \\left\\|\\epsilon_t - \\epsilon_\\theta(x_t,t) \\right\\|^2\n\\]\nThis means that the training procedure consists of only four steps:\n\nRandom sampling: Randomly sample a data point \\(x_0\\) from the training dataset.\nRandom timestep: Select a random time step $ t $.\nAdd noise in the forward process: Generate a noisy version of the data point \\(x_t\\) using the forward diffusion equation:\n\\[ x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0, I)\\]\nTrain the model: Use the noisy image \\(x_t\\) and the corresponding noise \\(\\epsilon_t\\) to train the neural network \\(\\epsilon_\\theta(x_t, t)\\) to predict the noise.\nLoss computation: Compute the loss using the simplified loss function:\n\\[ L_{simple} = \\left\\|\\epsilon_t - \\epsilon_\\theta(x_t,t) \\right\\|^2 \\]"
  }
]