{"title":"Diffusion Model","markdown":{"yaml":{"title":"Diffusion Model","author":"Yuyang","date":"2026-01-09","categories":["Deep Learning","Math"],"toc":true,"toc-location":"right","toc-depth":3,"bibliography":"references.bib","link-citations":true,"format":{"html":{"theme":"cosmo","code-fold":true,"math":"katex","highlight-style":"atom-one","css":"styles.css"}}},"headingText":"Diffusion Models","containsRefs":false,"markdown":"\n\n\n\n## What is Diffusion Model?\nDiffusion models are a class of image generative neural networks inspired by and formulated as a simulation of the thermodynamic diffusion process. The core idea is to model the data generation process as a gradual denoising process, where a neural network learns to reverse a diffusion process that progressively adds noise to the data until it becomes pure noise. By learning this reverse process, the model can generate new data samples from random noise.\n\n## SDE: Stochastic Differential Equations\nDiffusion models can be mathematically described using Stochastic Differential Equations (SDEs). @SohlDickstein2015DeepUL introduced the concept of diffusion probabilistic models, which can be viewed as a discrete-time approximation of continuous-time SDEs.\n\nA stochastic differential equation (SDE) in one-dimensional space can be expressed as:\n$$dx = f(x, t)dt + g(t)dW_t \\quad dW_t \\sim \\mathcal{N}(0, dt)$$\nwhere:\n- $x$ is the state variable (e.g., pixel values of an image).\n- $t$ is time.\n- $f(x, t)$ is the drift coefficient, representing the deterministic part of the evolution of $x$ over time.\n- $g(t)$ is the diffusion coefficient, representing the stochastic part of the evolution.\n- $dW_t$ is the increment of a Wiener process (or Brownian motion), which introduces randomness into the system.\n\nThe equation describes how the state variable $x$ evolves over time under the influence of both deterministic and stochastic components. The drift term $f(x, t)dt$ captures the systematic changes in $x$, while the diffusion term $g(t)dW_t$ captures the random fluctuations.\n\nIt is worth noting that when the deterministic component $f(x,t)$ is negatively correlated with the position variable $x$, the value of $x$ will gradually move toward the origin as time progresses. Meanwhile, due to the presence of the stochastic term, $x$ will eventually undergo random motion around the origin, following a Gaussian distribution.\n\n## From SDE to Diffusion Models\nIn diffusion models, the forward diffusion process can be described by a specific type of SDE. The SDE is defined as:\n$$dx = -\\frac{1}{2}\\beta(t)xdt + \\sqrt{\\beta(t)}dW_t\\quad dW_t \\sim \\mathcal{N}(0, dt)$$\nwhere $\\beta(t)$ is a time-dependent function that controls the rate of noise addition over time.\n\nSince $dW_t$ follows a Gaussian distribution, we can discretize the SDE over small time intervals $\\Delta t$ to obtain the following discrete-time update equation:\n$$x_{t+\\Delta t} = x_t - \\frac{1}{2}\\beta(t)\\cdot x_t \\cdot dt + \\sqrt{\\beta(t)\\Delta t}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\nwhere $\\epsilon_t$ is a standard Gaussian noise term.\n\nThe following formula can be obtained from the Euler approximation and the Taylor expansion.\n$$\nx_i = \\sqrt{1-\\beta_i}x_{i-1} + \\sqrt{\\beta_i}\\epsilon_{i-1}, \\quad \\epsilon_{i-1} \\sim \\mathcal{N}(0, I) \n$${#eq-eq1}\n\n::: {.callout-note collapse=\"true\"}\n## Euler-Maruyama and the Taylor expansion Method\npass\n\n:::\n\n### Forward single-step diffusion process\nLet $\\beta = 1 - \\alpha$ in @eq-eq1. We can then derive the formula for $x_i$ corresponding to a given $x_{i-1}$. Since $\\alpha = [0.99999, \\dots, 0.999]$, at each time step, $x_{t-1}$ is scaled by $\\sqrt{\\alpha_t}$ and some noise is added. Because the noise follows a Gaussian distribution, conditioned on $x_{t-1}$, $x_t$ also follows a Gaussian distribution $N$. Then, a random sample from the distribution of $x_i$ is used as the input for determining $x_{t+1}$, and this process is iterated step by step. As a result, at each time step, $x_t$ moves closer to the origin. When $T$ is sufficiently large, i.e., there are enough time steps, the final $x_i$ follows a standard normal distribution: $x_t \\sim N(0,1)$.\n$$\n\\left\\{\n\\begin{aligned}\n  x_t = \\sqrt{\\alpha_{t}} \\cdot x_{t-1} + \\sqrt{ 1- \\alpha_{t}} \\cdot \\epsilon_{t-1} \\\\\n  p(x_t|x_{t-1}) \\sim \\mathcal{N}(\\sqrt{\\alpha_{t}} \\cdot x_{t-1}, 1- \\alpha_{t})\n\\end{aligned}\n\\right.\n$${#eq-eq2}\nwhere $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$.\n\n![](figures/2015singlestep.png)\n\n### Multi-step forward diffusion process\nBy recursively applying the single-step diffusion process in @eq-eq2, we can derive the multi-step forward diffusion process. Specifically, we can express $x_t$ directly in terms of the original data point $x_0$ and the accumulated noise over $t$ steps. This leads to the following equations:\n$$\n\\left\\{\n\\begin{aligned}\n  x_t = \\sqrt{\\bar{\\alpha}_{t}} \\cdot x_0 + \\sqrt{ 1- \\bar{\\alpha}_{t}} \\cdot \\epsilon_t \\\\\n  p(x_t|x_0) \\sim \\mathcal{N}(\\sqrt{\\bar{\\alpha}_{t}} \\cdot x_0, 1- \\bar{\\alpha}_{t})\n\\end{aligned}\n\\right.\n$${#eq-eq3}\nwhere $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$.\n![](figures/2015multistep.png)\n\n### Single-step reverse diffusion process\nThe reverse diffusion process aims to recover the original data point $x_0$ from the noisy observation $x_t$. @eq-eq1 gives us the single-step forward diffusion process, but only in the very first step, that is, when $x_0$ is considered, $x_{t-1}$ is a deterministic value. However, in subsequent steps, $x_{t-1}$ is sampled from a probability distribution, and $\\epsilon$ is also a stochastic value. The computation of $x_t$ involves combining these two uncertain values through scaling and addition. A single $x_t$ can correspond to infinitely many possible pairs of $x_{i-t}$ and $\\epsilon_t$. This means that the neural network would need to learn two uncertain values simultaneously, which is something the network cannot achieve.\n\nFor @eq-eq2, we can derive the reverse process as follows:\n$$x_0 = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_{t}}\\epsilon_t}{\\sqrt{\\bar{\\alpha}_{t}}}$$\nSince $\\sqrt{\\bar{\\alpha}_{t}}$ is nearly zero, our network does not have sufficent precision.\n\nHowever, if we know the starting point $x_0$ and the current point $x_t$, the intermediate point $x_{t-1}$ becomes a completely determined Gaussian distribution. Its closed-form expression can be derived as follows:\n$$q(x_{t-1}|x_t, x_0) = \\mathcal{N}\\left(x_{t-1};  \\textcolor{red}{\\tilde{\\mu}_t(x_t, x_0)}, \\textcolor{blue}{\\tilde{\\beta}_t I}\\right)$$\nwhere\n$$\n\\begin{aligned}\n\\textcolor{red}{\\tilde{\\mu}_t(x_t, x_0)} &= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} x_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t} x_0 \\\\\n&= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} x_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t} (\\frac{x_t - \\sqrt{1 - \\bar{\\alpha}}\\epsilon_t}{\\sqrt{\\bar{\\alpha}_t}}) \\\\\n&= \\textcolor{red}{\\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_t \\right)} \\\\\n\\end{aligned}\n$${#eq-eq4}\n$$\n\\textcolor{blue}{\\tilde{\\beta}_t} = \\textcolor{blue}{\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t}\n$${#eq-eq5}\n\n:::{.callout-note collapse=\"true\"}\n## The derivation of $\\tilde{\\mu}_t$ and $\\tilde{\\beta}_t$\n\nTo derive the expressions for $\\tilde{\\mu}_t$ and $\\tilde{\\beta}_t$, we can apply Bayes' rule.\n<div style=\"font-size:0.8em; text-align: left\">\n\n\\begin{aligned}\nq(x_{t-1}|x_t, x_0) &= q(x_t|x_{t-1})\\frac{q(x_{t-1}|x_0)}{q(x_{t}|x_0)} \\\\\n&\\propto \\exp \\Big(-\\frac{1}{2}(\\frac{\\left(x_t - \\sqrt{\\alpha_t}x_{t-1}\\right)^2}{\\beta_t} + \\frac{\\left(x_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}}x_0\\right)^2}{1-\\bar{\\alpha}_{t-1}} - \\frac{\\left(x_t - \\sqrt{\\alpha_t}x_0\\right)^2}{1 - \\bar{\\alpha}_t})\\Big) \\\\\n&= exp \\Big(-\\frac{1}{2} \\Big( (\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1-\\bar{\\alpha}_{t-1}})x_{t-1}^2 - 2(\\frac{\\sqrt{\\alpha_t}x_t}{\\beta_t} + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}x_0}{1-\\bar{\\alpha}_{t-1}})x_{t-1} + C(x_t, x_0) \\Big) \\Big) \\\\\n\n\\end{aligned}\n</div>\n\nAccording the standard form of Gaussian distribution, we can identify the mean and variance as follows:\n<div style=\"font-size:0.8em; text-align: left\">\n\n$$\n\\textcolor{red}{\\tilde{\\mu}_t(x_t, x_0)} = (\\frac{\\sqrt{\\alpha_t}x_t}{\\beta_t} + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}x_0}{1-\\bar{\\alpha}_{t-1}})/(\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1-\\bar{\\alpha}_{t-1}})$$\n\n$$\n\\textcolor{blue}{\\tilde{\\beta}_t} = 1/(\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1-\\bar{\\alpha}_{t-1}}) = \\textcolor{blue}{\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t}$$\n\n</div>\n\nBased on the forward process formula $x_t = \\sqrt(\\bar{\\alpha}_{t})x_0 + \\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon_t$, we can derive it as:\n<div style=\"font-size:0.8em; text-align: left\">\n$$ \nx_0 = \\frac{x_t-\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon}{\\sqrt{\\bar{\\alpha}_{t}}}\n$$\n</div>\nBy substituting this $x_0$ into the formula for $\\tilde{\\mu}_t$ and simplifying, we get\n<div style=\"font-size:0.8em; text-align: left\">\n$$\n\\textcolor{red}{\\mu_t(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1-\\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon(x_t, t) \\right)}\n$$\n</div>\n\n:::\n\n## DDPM: Denoising Diffusion Probabilistic Models\n\n\n\n### Forward diffusion process\nThe forward diffusion process is essentially as Markov chain that gradually adds Gaussian noise to the data over a series of time steps until the data is completely transformed into pure random noise.\n\n### Single-step noise addition\nGiven a data point $x_0 \\sim q(x)$ from the data distribution, the forward diffusion process adds noise in $T$ discrete time steps. At each time step $t$, Gaussian noise is added to the data point according to a variance schedule $\\beta_t = (\\beta_1, \\beta_2, \\ldots, \\beta_T)$. The noise addition at each step can be described by the following equation:\n$$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I)$$\n\nwhere $\\beta_t$ is the variance schedule at time step $t$,which means at each time step, how much Gaussian noise is added to the data point and normally is chosen to be a small positive value. Meanswhile, $I$ is the identity matrix, ensuring that the noise is isotropic (i.e., the same in all directions). Expected mean of $x_t$ is scaled version of $x_{t-1}$ by $\\sqrt{1-\\beta_t}$, which ensures that as more noise is added, the original data point's influence diminishes. \n\nIf we use the reparameterization trick, we can directly express the relation between $x_t$ and $x_{t-1}$ and further more between $x_t$ and $x_0$:\n$$x_t = \\sqrt{1-\\beta_t}x_{t-1} + \\sqrt{\\beta_t}\\epsilon_{t-1}, \\quad \\epsilon_{t-1} \\sim \\mathcal{N}(0, I)$$\n$$x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\nwhere $\\alpha_t = 1 - \\beta_t$ and $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$.\n\n\n::: {.callout-note collapse=\"true\"}\n## Conversion from a distribution to an equation (The Reparameterization)\n\nIn the paper @ho2020denoising, the authors define the forward diffusion process using a Gaussian distribution:\n$$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\underbrace{\\sqrt{1-\\beta_t}x_{t-1}}_{\\text{mean}\\,\\mu}, \\underbrace{\\beta_t I}_{\\text{variance}\\,\\sigma^2})$$\nIt means $x_t \\sim \\mathcal{N}(\\mu,\\sigma^2)$, and any variable that follows a Gaussian distribution can be expressed as: $$ X = \\mu + \\sigma \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I) $$\n\nIn our case, we have:\n$$\n\\mu = \\sqrt{1-\\beta_t}x_{t-1}, \\quad \\sigma = \\sqrt{\\beta_t}\n$$\nThus, we can rewrite the forward diffusion step as:\n$$\n\\begin{aligned}\nx_t = \\mu + \\sigma \\odot \\epsilon = \\sqrt{1-\\beta_t}x_{t-1} + \\sqrt{\\beta_t}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})\n\\end{aligned}\n$$\n\nOtherwise, we can also express $x_t$ directly in terms of $x_0$:\n$$\n\\begin{aligned}\nx_t &= \\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\\\\n&= \\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1}}x_{t-2} + \\sqrt{1-\\alpha_{t-1}}\\epsilon_{t-2}) + \\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\\\\n&= \\sqrt{\\alpha_t \\alpha_{t-1}}x_{t-2} + \\sqrt{\\alpha_t(1-\\alpha_{t-1})}\\epsilon_{t-2} + \\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\\\\n\\end{aligned}\n$$\n\nSince $\\epsilon_{t-2}$ and $\\epsilon_{t-1}$ are both independent and identically distributed as $\\mathcal{N}(0, I)$, according to the additivity property of Gaussian distributions:\n$$\\mathcal{N}(\\mu_1, \\sigma_1^2) + \\mathcal{N}(\\mu_2, \\sigma_2^2) = \\mathcal{N}(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)$$\nWe can combine the noise terms:\n$$\n\\sqrt{\\alpha_t(1-\\alpha_{t-1})}\\epsilon_{t-2} + \\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\sim \\mathcal{N}\\left(0, \\alpha_t(1-\\alpha_{t-1})I + (1-\\alpha_t)I\\right) = \\mathcal{N}\\left(0, (1 - \\alpha_t \\alpha_{t-1})I\\right)\n$$\nTherefore:\n$$\n\\begin{aligned}\nx_t &= \\sqrt{\\alpha_t \\alpha_{t-1}}x_{t-2} + \\sqrt{1 - \\alpha_t \\alpha_{t-1}}\\bar{\\epsilon}\\\\\n&=\\dots \\\\\n&= \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\mathbf{I}) \\\\\n\\end{aligned}\n$$\nwhere $\\alpha_t = 1 - \\beta_t$ and $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$.\n:::\n\n::: {.callout-note collapse=\"true\"}\n### What is reparameterization and Why use it?\nIn deep generative models, we often need to sample from a probability distribution that depends on some parameters. For example, in diffusion models, a neural network predicts the mean $\\mu$ and variance $\\sigma^2$ of a Gaussian distribution at each time step, and then we need to sample a variable $x_t$ from $\\mathcal{N}(\\mu, \\sigma^2)$.\n\nHowever, sampling directly from this distribution can make it difficult to compute gradients with respect to the parameters during training. Suppose our neural network has parameters $\\theta$, and we want to compute the gradient of some loss function $L$ with respect to $\\theta$. If we sample $x_t$ directly from $\\mathcal{N}(\\mu_\\theta, \\sigma^2_\\theta)$, the sampling operation is not a function, which introduces randomness that breaks the gradient flow, making it impossible to compute $\\nabla_\\theta L$. The fatal problem is that the sampling operation is not differentiable. \n\nThe core idea of the reparameterization trick is to seperate the randomness from the parameters by expressing the random variable $x_t$ as a deterministic function of the parameters and an independent noise variable. For a Gaussian distribution, we can express the sampling operation as:$$x_t = \\mu_\\theta + \\sigma_\\theta \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 1)$$\nHere, $\\epsilon \\sim \\mathcal{N}(0, 1)$, which is a random variable drawn from a standard normal distribution and independent of the parameters $\\theta$. This way, the randomness is isolated in $\\epsilon$, and the rest of the expression is a deterministic function of $\\theta$. It can be described by the following: $$x_{t} = \\mu_{\\theta} + \\sigma_{\\theta} \\odot \\epsilon$$\n\nNow, we can sample $x_t$ by sampling $\\epsilon$ from a fixed distribution (standard normal) and then applying the deterministic transformation. This allows us to compute gradients with respect to $\\theta$ during backpropagation, as the sampling operation is now differentiable with respect to the parameters, which $\\nabla_\\mu x_{t} = 1$ and $\\nabla_\\sigma x_{t} = \\epsilon$.\n\nWe also can compare two methods below in Pytorch:\n\n```python\n\nimport torch\n\nmu = torch.tensor([1.0], requires_grad=True)\nsigma = torch.tensor([0.5], requires_grad=True)\n# Direct sampling (not differentiable)\ndistribution = torch.distributions.Normal(mu, sigma)\nx_direct = distribution.sample()  # sample() for non-differentiable case\n# x_direct = distribution.rsample()  # rsample() for reparameterized case\nloss_direct = (x_direct - 2.0) ** 2\n# loss_direct.backward()  # This would fail to compute gradients, if using rsample(), it works\n\n# Reparameterization trick (differentiable)\nepsilon = torch.randn(1)\nx_reparam = mu + sigma * epsilon  # reparameterized sample\nloss_reparam = (x_reparam - 2.0) ** 2\nloss_reparam.backward()  # This works\nprint(mu.grad)  # Gradient with respect to mu\nprint(sigma.grad)  # Gradient with respect to sigma\n\n```\n:::\n\nWe can implement the forward process using PyTorch as follows:\n\n## Reverse diffusion process\nIn the process of forward diffusion, we defined how to gradually add Gaussian noise to the data using $q(x_t|x_{t-1})$. The reverse diffusion process aims to reverse this noising process, gradually removing the noise to recover the original data from pure noise by using its reverse conditional distribution $q(x_{t-1}|x_t)$. If we can accurately model this reverse process, we can start from random noise $x_T \\sim \\mathcal{N}(0, I)$ and iteratively sample $x_{T-1}, x_{T-2}, \\ldots, x_0$ to generate new data samples that resemble the original data distribution.\n\n### Can we directly compute the reverse conditional distribution？\nIn theory, if we have access to the true data distribution and the forward process, we can compute the reverse conditional distribution $q(x_{t-1}|x_t)$ using Bayes' theorem:\n$$q(x_{t-1}|x_t) = \\frac{q(x_t|x_{t-1})q(x_{t-1})}{q(x_t)}$$\nHowever, in practice, this is infeasible for several reasons:\n\n1. **Unknown Data Distribution**: We typically do not have access to the true data distribution $q(x - 1)$. The reason is that we only have a finite dataset of samples from the data distribution, not the distribution itself.\n\n2. **Intractable Integrals**: The denominator $q(x_t)$ is referred to as the marginal distribution of $x_t$. If we want to compute the probability at a specific time step $t$, we need to integrate over all possible previous states $x_{t-1}$:\n   $$q(x_t) = \\int q(x_t|x_{t-1})q(x_{t-1})dx_{t-1}$$\nThis integral is often intractable, especially in high-dimensional spaces, which is called the \"curse of dimensionality\".\n\nDue to these challenges, we cannot directly compute $q(x_{t-1}|x_t)$ and instead need to approximate it using a parameterized model, such as a neural network.\n\n### Using a neural network to approximate the reverse process\nWe can use a neural network with parameters $\\theta$ to approximate the reverse conditional distribution. When the diffusion step size $\\beta_t$ is sufficiently small, the reverse process $q(x_{t-1} \\mid x_t)$ also follows a Gaussian distribution. Therefore, we can define the neural network outputs as the parameters of this Gaussian distribution:\n$$ \np_\\theta(x_{0:T}) = p(x_T) \\prod_{t=1}^{T} p_\\theta(x_{t-1}|x_t) \\quad\np_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))\n$$\nwhere $\\mu_\\theta(x_t, t)$ and $\\Sigma_\\theta(x_t, t)$ are the mean and covariance predicted by the neural network for the reverse step from $x_t$ to $x_{t-1}$. \n\nWhat is more, in the paper @ho2020denoising, the authors find that using a fixed covariance $\\Sigma_\\theta(x_t, t) = \\beta_t I$ works well in practice, simplifying the model to only predict the mean $\\mu_\\theta(x_t, t)$, which is the main focus of the training process.\n\nNow, we can train the neural network to learn the parameters $\\theta$ such that the reverse process $p_\\theta(x_{t-1}|x_t)$ closely approximates the true but unknow reverse conditional distribution $q(x_{t-1}|x_t)$. To train the model, we aim to maximize the log-likelihood of the real data $x_0$ under the model, $\\log p_\\theta(x_0)$. Because direct likelihood optimization is intractable, we resort to Jensen's inequality and optimize the corresponding Evidence Lower Bound (ELBO):\n$$- \\log p_\\theta(x_0) \\le \\mathbb{E}_{q} \\left[ \\log \\frac{q(x_{1:T}|x_0)}{p_\\theta(x_{0:T})} \\right] = \\text{ELBO}$$\n\nTo convert each term in the equation to be analytically computable, the objective can be further rewritten to be a combination of several KL-divergence and entropy terms (See the detailed step-by-step process in @sohldickstein2015deepunsupervisedlearningusing):\n$$\\small \\text{ELBO} = \\mathbb{E}_{q} \\left[ \\underbrace{D_{KL}(q(x_T|x_0) || p_\\theta(x_T))}_{L_T}  + \\sum_{t=2}^{T} \\underbrace{D_{KL}(q(x_{t-1}|x_t, x_0) || p_\\theta(x_{t-1}|x_t))}_{L_{t-1}} \\underbrace{- \\log p_\\theta(x_0|x_1)}_{L_0} \\right]$$\nwhere $D_{KL}(P||Q)$ is the Kullback-Leibler divergence between two distributions $P$ and $Q$.\n\nAlthough the $q(x_{t-1}|x_t)$ is intractable, $q(x_{t-1}|x_t, x_0)$ is tractable because we condition on the original data point $x_0$. While $x_0$ is unknown at inference time, it is available during training as part of the dataset. Following the @eq-eq4 and @eq-eq5, we can derive the closed-form expression for $q(x_{t-1}|x_t, x_0)$ as a Gaussian distribution with mean $\\tilde{\\mu}_t(x_t, x_0)$ and variance $\\tilde{\\beta}_t I$.\n\n\nNow, our current goal is to have the neural network $p_\\theta \\sim \\mathcal{N}\\left(\\mu_\\theta,\\Sigma_\\theta\\right)$ approximate the true distribution $q(x_{t-1}|x_t, x_0) \\sim \\mathcal{N}\\left(\\tilde{\\mu}_t, \\tilde{\\beta}_t\\right)$. Since the parameter $\\Sigma_\\theta$ is often fixed as $\\beta_t I$, we mainly focus on training the neural network to predict the mean $\\textcolor{red}{\\mu_\\theta}$ to be as close as possible to the true mean $\\textcolor{red}{\\tilde{\\mu}_t = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_t \\right)}$.\n\nThe equation means that to accurately predict the mean $\\mu_\\theta$, the neural network only needs to predict the noise $\\epsilon_\\theta$ present in the current image.\n\nThis can be achieved by minimizing the KL-divergence between these two distributions:\n\n$$\n\\begin{aligned}\nL_{t} &= \\mathbb{E}_{x_0,\\epsilon} \\left[D_{KL}(q(x_{t-1}|x_t, x_0) || p_\\theta(x_{t-1}|x_t))\\right]\\\\\n&= \\mathbb{E}_{x_0,\\epsilon} \\left[\\frac{1}{2\\|\\Sigma_{\\theta}(x_t,t)\\|_2 ^ 2}\\| \\tilde{\\mu}_t(x_t, x_0) - \\mu_\\theta(x_t, t) \\|^2\\right]\\\\\n&= \\mathbb{E}_{x_0,\\epsilon} \\left[\\frac{1}{2\\|\\Sigma_{\\theta}(x_t,t)\\|_2 ^ 2}\\| \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_t \\right) - \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right) \\|^{2}\\right]\\\\\n&= \\mathbb{E}_{x_0,\\epsilon} \\left[\\frac{(1-\\alpha_t)^2}{2\\alpha_t(1-\\bar{\\alpha}_t)\\| \\Sigma_\\theta \\|_2^2}\n\\left\\lVert\n\\epsilon_t - \\epsilon_\\theta(x_t,t)\n\\right\\rVert^2\n\\right] \\\\\n&= \\mathbb{E}_{x_0,\\epsilon}\n\\left[\n\\frac{(1-\\alpha_t)^2}\n{2\\alpha_t(1-\\bar{\\alpha}_t)\\left\\lVert \\Sigma_\\theta \\right\\rVert _2^2}\n\\left\\lVert\n\\epsilon_t -\n\\epsilon_\\theta\\!\\left(\n\\sqrt{\\bar{\\alpha}_t} x_0 +\n\\sqrt{1-\\bar{\\alpha}_t}\\,\\epsilon_t,\nt\n\\right)\n\\right\\rVert^2\n\\right]\n\\end{aligned}\n$$\n\n\n@ho2020denoising further simplifies the loss function by removing the weighting term, leading to a more straightforward objective:\n\n$$\n\\boxed {\\textcolor{red}{L_t^{simple} = \\mathbb{E}_{x_0,\\epsilon,t} \\left[\\left\\lVert \\epsilon_t - \\epsilon_\\theta(x_t,t) \\right\\rVert^2\\right]}}\n$$\n\nThe final training objective is:\n\n$$\nL_{simple} = L_t^{simple} + C\n$$\n\nwhere $C$ is a constant that does not depend on the model parameters $\\theta$ and can be ignored during optimization.\n\n![](figures/DDPM-algo.png)\n\n### Sampling from the trained model\nOnce the neural network which can predict the noise $\\epsilon_\\theta(x_t, t)$ is trained, we can use it to sample new data points by reversing the diffusion process. Starting from pure Gaussian noise $x_T \\sim \\mathcal{N}(0, I)$, we iteratively apply the reverse diffusion steps using the learned model.\n\n$$\nx_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right) + \\sigma_t z\n$$\n\nwhere $z \\sim \\mathcal{N}(0, I)$ is random noise added at each step to maintain stochasticity, and $\\sigma_t$ is a scaling factor that can be adjusted based on the desired level of noise during sampling.\n\n::: {.callout-note}\n## Why we need to add $\\sigma_t z$？\nThis term corresponds to Langevin dynamics. Without it, the sampling process becomes a deterministic numerical procedure, which often leads to overly smooth images with a lack of fine details. Introducing stochastic perturbations helps correct estimation errors at each step and enables the generative model to better explore the true image manifold.\n\n:::\n\n### How to train the model\nWe get the simplified loss function as follows:\n\n$$\nL_{simple} = \\left\\|\\epsilon_t - \\epsilon_\\theta(x_t,t) \\right\\|^2\n$$\n\nThis means that the training procedure consists of only four steps:\n\n1. **Random sampling**: Randomly sample a data point $x_0$ from the training dataset.\n\n2. **Random timestep**: Select a random time step $ t \\in [1, T]$.\n\n3. **Add noise in the forward process**: Generate a noisy version of the data point $x_t$ using the forward diffusion equation:\n\n   $$ x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0, I)$$\n\n4. **Train the model**: Use the noisy image $x_t$ and the corresponding noise $\\epsilon_t$ to train the neural network $\\epsilon_\\theta(x_t, t)$ to predict the noise.\n\n5. **Loss computation**: Compute the loss using the simplified loss function:\n\n   $$ L_{simple} = \\left\\|\\epsilon_t - \\epsilon_\\theta(x_t,t) \\right\\|^2 $$\n   \n\n","srcMarkdownNoYaml":"\n\n\n# Diffusion Models\n\n## What is Diffusion Model?\nDiffusion models are a class of image generative neural networks inspired by and formulated as a simulation of the thermodynamic diffusion process. The core idea is to model the data generation process as a gradual denoising process, where a neural network learns to reverse a diffusion process that progressively adds noise to the data until it becomes pure noise. By learning this reverse process, the model can generate new data samples from random noise.\n\n## SDE: Stochastic Differential Equations\nDiffusion models can be mathematically described using Stochastic Differential Equations (SDEs). @SohlDickstein2015DeepUL introduced the concept of diffusion probabilistic models, which can be viewed as a discrete-time approximation of continuous-time SDEs.\n\nA stochastic differential equation (SDE) in one-dimensional space can be expressed as:\n$$dx = f(x, t)dt + g(t)dW_t \\quad dW_t \\sim \\mathcal{N}(0, dt)$$\nwhere:\n- $x$ is the state variable (e.g., pixel values of an image).\n- $t$ is time.\n- $f(x, t)$ is the drift coefficient, representing the deterministic part of the evolution of $x$ over time.\n- $g(t)$ is the diffusion coefficient, representing the stochastic part of the evolution.\n- $dW_t$ is the increment of a Wiener process (or Brownian motion), which introduces randomness into the system.\n\nThe equation describes how the state variable $x$ evolves over time under the influence of both deterministic and stochastic components. The drift term $f(x, t)dt$ captures the systematic changes in $x$, while the diffusion term $g(t)dW_t$ captures the random fluctuations.\n\nIt is worth noting that when the deterministic component $f(x,t)$ is negatively correlated with the position variable $x$, the value of $x$ will gradually move toward the origin as time progresses. Meanwhile, due to the presence of the stochastic term, $x$ will eventually undergo random motion around the origin, following a Gaussian distribution.\n\n## From SDE to Diffusion Models\nIn diffusion models, the forward diffusion process can be described by a specific type of SDE. The SDE is defined as:\n$$dx = -\\frac{1}{2}\\beta(t)xdt + \\sqrt{\\beta(t)}dW_t\\quad dW_t \\sim \\mathcal{N}(0, dt)$$\nwhere $\\beta(t)$ is a time-dependent function that controls the rate of noise addition over time.\n\nSince $dW_t$ follows a Gaussian distribution, we can discretize the SDE over small time intervals $\\Delta t$ to obtain the following discrete-time update equation:\n$$x_{t+\\Delta t} = x_t - \\frac{1}{2}\\beta(t)\\cdot x_t \\cdot dt + \\sqrt{\\beta(t)\\Delta t}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\nwhere $\\epsilon_t$ is a standard Gaussian noise term.\n\nThe following formula can be obtained from the Euler approximation and the Taylor expansion.\n$$\nx_i = \\sqrt{1-\\beta_i}x_{i-1} + \\sqrt{\\beta_i}\\epsilon_{i-1}, \\quad \\epsilon_{i-1} \\sim \\mathcal{N}(0, I) \n$${#eq-eq1}\n\n::: {.callout-note collapse=\"true\"}\n## Euler-Maruyama and the Taylor expansion Method\npass\n\n:::\n\n### Forward single-step diffusion process\nLet $\\beta = 1 - \\alpha$ in @eq-eq1. We can then derive the formula for $x_i$ corresponding to a given $x_{i-1}$. Since $\\alpha = [0.99999, \\dots, 0.999]$, at each time step, $x_{t-1}$ is scaled by $\\sqrt{\\alpha_t}$ and some noise is added. Because the noise follows a Gaussian distribution, conditioned on $x_{t-1}$, $x_t$ also follows a Gaussian distribution $N$. Then, a random sample from the distribution of $x_i$ is used as the input for determining $x_{t+1}$, and this process is iterated step by step. As a result, at each time step, $x_t$ moves closer to the origin. When $T$ is sufficiently large, i.e., there are enough time steps, the final $x_i$ follows a standard normal distribution: $x_t \\sim N(0,1)$.\n$$\n\\left\\{\n\\begin{aligned}\n  x_t = \\sqrt{\\alpha_{t}} \\cdot x_{t-1} + \\sqrt{ 1- \\alpha_{t}} \\cdot \\epsilon_{t-1} \\\\\n  p(x_t|x_{t-1}) \\sim \\mathcal{N}(\\sqrt{\\alpha_{t}} \\cdot x_{t-1}, 1- \\alpha_{t})\n\\end{aligned}\n\\right.\n$${#eq-eq2}\nwhere $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$.\n\n![](figures/2015singlestep.png)\n\n### Multi-step forward diffusion process\nBy recursively applying the single-step diffusion process in @eq-eq2, we can derive the multi-step forward diffusion process. Specifically, we can express $x_t$ directly in terms of the original data point $x_0$ and the accumulated noise over $t$ steps. This leads to the following equations:\n$$\n\\left\\{\n\\begin{aligned}\n  x_t = \\sqrt{\\bar{\\alpha}_{t}} \\cdot x_0 + \\sqrt{ 1- \\bar{\\alpha}_{t}} \\cdot \\epsilon_t \\\\\n  p(x_t|x_0) \\sim \\mathcal{N}(\\sqrt{\\bar{\\alpha}_{t}} \\cdot x_0, 1- \\bar{\\alpha}_{t})\n\\end{aligned}\n\\right.\n$${#eq-eq3}\nwhere $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$.\n![](figures/2015multistep.png)\n\n### Single-step reverse diffusion process\nThe reverse diffusion process aims to recover the original data point $x_0$ from the noisy observation $x_t$. @eq-eq1 gives us the single-step forward diffusion process, but only in the very first step, that is, when $x_0$ is considered, $x_{t-1}$ is a deterministic value. However, in subsequent steps, $x_{t-1}$ is sampled from a probability distribution, and $\\epsilon$ is also a stochastic value. The computation of $x_t$ involves combining these two uncertain values through scaling and addition. A single $x_t$ can correspond to infinitely many possible pairs of $x_{i-t}$ and $\\epsilon_t$. This means that the neural network would need to learn two uncertain values simultaneously, which is something the network cannot achieve.\n\nFor @eq-eq2, we can derive the reverse process as follows:\n$$x_0 = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_{t}}\\epsilon_t}{\\sqrt{\\bar{\\alpha}_{t}}}$$\nSince $\\sqrt{\\bar{\\alpha}_{t}}$ is nearly zero, our network does not have sufficent precision.\n\nHowever, if we know the starting point $x_0$ and the current point $x_t$, the intermediate point $x_{t-1}$ becomes a completely determined Gaussian distribution. Its closed-form expression can be derived as follows:\n$$q(x_{t-1}|x_t, x_0) = \\mathcal{N}\\left(x_{t-1};  \\textcolor{red}{\\tilde{\\mu}_t(x_t, x_0)}, \\textcolor{blue}{\\tilde{\\beta}_t I}\\right)$$\nwhere\n$$\n\\begin{aligned}\n\\textcolor{red}{\\tilde{\\mu}_t(x_t, x_0)} &= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} x_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t} x_0 \\\\\n&= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} x_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t} (\\frac{x_t - \\sqrt{1 - \\bar{\\alpha}}\\epsilon_t}{\\sqrt{\\bar{\\alpha}_t}}) \\\\\n&= \\textcolor{red}{\\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_t \\right)} \\\\\n\\end{aligned}\n$${#eq-eq4}\n$$\n\\textcolor{blue}{\\tilde{\\beta}_t} = \\textcolor{blue}{\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t}\n$${#eq-eq5}\n\n:::{.callout-note collapse=\"true\"}\n## The derivation of $\\tilde{\\mu}_t$ and $\\tilde{\\beta}_t$\n\nTo derive the expressions for $\\tilde{\\mu}_t$ and $\\tilde{\\beta}_t$, we can apply Bayes' rule.\n<div style=\"font-size:0.8em; text-align: left\">\n\n\\begin{aligned}\nq(x_{t-1}|x_t, x_0) &= q(x_t|x_{t-1})\\frac{q(x_{t-1}|x_0)}{q(x_{t}|x_0)} \\\\\n&\\propto \\exp \\Big(-\\frac{1}{2}(\\frac{\\left(x_t - \\sqrt{\\alpha_t}x_{t-1}\\right)^2}{\\beta_t} + \\frac{\\left(x_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}}x_0\\right)^2}{1-\\bar{\\alpha}_{t-1}} - \\frac{\\left(x_t - \\sqrt{\\alpha_t}x_0\\right)^2}{1 - \\bar{\\alpha}_t})\\Big) \\\\\n&= exp \\Big(-\\frac{1}{2} \\Big( (\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1-\\bar{\\alpha}_{t-1}})x_{t-1}^2 - 2(\\frac{\\sqrt{\\alpha_t}x_t}{\\beta_t} + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}x_0}{1-\\bar{\\alpha}_{t-1}})x_{t-1} + C(x_t, x_0) \\Big) \\Big) \\\\\n\n\\end{aligned}\n</div>\n\nAccording the standard form of Gaussian distribution, we can identify the mean and variance as follows:\n<div style=\"font-size:0.8em; text-align: left\">\n\n$$\n\\textcolor{red}{\\tilde{\\mu}_t(x_t, x_0)} = (\\frac{\\sqrt{\\alpha_t}x_t}{\\beta_t} + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}x_0}{1-\\bar{\\alpha}_{t-1}})/(\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1-\\bar{\\alpha}_{t-1}})$$\n\n$$\n\\textcolor{blue}{\\tilde{\\beta}_t} = 1/(\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1-\\bar{\\alpha}_{t-1}}) = \\textcolor{blue}{\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t}$$\n\n</div>\n\nBased on the forward process formula $x_t = \\sqrt(\\bar{\\alpha}_{t})x_0 + \\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon_t$, we can derive it as:\n<div style=\"font-size:0.8em; text-align: left\">\n$$ \nx_0 = \\frac{x_t-\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon}{\\sqrt{\\bar{\\alpha}_{t}}}\n$$\n</div>\nBy substituting this $x_0$ into the formula for $\\tilde{\\mu}_t$ and simplifying, we get\n<div style=\"font-size:0.8em; text-align: left\">\n$$\n\\textcolor{red}{\\mu_t(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1-\\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon(x_t, t) \\right)}\n$$\n</div>\n\n:::\n\n## DDPM: Denoising Diffusion Probabilistic Models\n\n\n\n### Forward diffusion process\nThe forward diffusion process is essentially as Markov chain that gradually adds Gaussian noise to the data over a series of time steps until the data is completely transformed into pure random noise.\n\n### Single-step noise addition\nGiven a data point $x_0 \\sim q(x)$ from the data distribution, the forward diffusion process adds noise in $T$ discrete time steps. At each time step $t$, Gaussian noise is added to the data point according to a variance schedule $\\beta_t = (\\beta_1, \\beta_2, \\ldots, \\beta_T)$. The noise addition at each step can be described by the following equation:\n$$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I)$$\n\nwhere $\\beta_t$ is the variance schedule at time step $t$,which means at each time step, how much Gaussian noise is added to the data point and normally is chosen to be a small positive value. Meanswhile, $I$ is the identity matrix, ensuring that the noise is isotropic (i.e., the same in all directions). Expected mean of $x_t$ is scaled version of $x_{t-1}$ by $\\sqrt{1-\\beta_t}$, which ensures that as more noise is added, the original data point's influence diminishes. \n\nIf we use the reparameterization trick, we can directly express the relation between $x_t$ and $x_{t-1}$ and further more between $x_t$ and $x_0$:\n$$x_t = \\sqrt{1-\\beta_t}x_{t-1} + \\sqrt{\\beta_t}\\epsilon_{t-1}, \\quad \\epsilon_{t-1} \\sim \\mathcal{N}(0, I)$$\n$$x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\nwhere $\\alpha_t = 1 - \\beta_t$ and $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$.\n\n\n::: {.callout-note collapse=\"true\"}\n## Conversion from a distribution to an equation (The Reparameterization)\n\nIn the paper @ho2020denoising, the authors define the forward diffusion process using a Gaussian distribution:\n$$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\underbrace{\\sqrt{1-\\beta_t}x_{t-1}}_{\\text{mean}\\,\\mu}, \\underbrace{\\beta_t I}_{\\text{variance}\\,\\sigma^2})$$\nIt means $x_t \\sim \\mathcal{N}(\\mu,\\sigma^2)$, and any variable that follows a Gaussian distribution can be expressed as: $$ X = \\mu + \\sigma \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I) $$\n\nIn our case, we have:\n$$\n\\mu = \\sqrt{1-\\beta_t}x_{t-1}, \\quad \\sigma = \\sqrt{\\beta_t}\n$$\nThus, we can rewrite the forward diffusion step as:\n$$\n\\begin{aligned}\nx_t = \\mu + \\sigma \\odot \\epsilon = \\sqrt{1-\\beta_t}x_{t-1} + \\sqrt{\\beta_t}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})\n\\end{aligned}\n$$\n\nOtherwise, we can also express $x_t$ directly in terms of $x_0$:\n$$\n\\begin{aligned}\nx_t &= \\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\\\\n&= \\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1}}x_{t-2} + \\sqrt{1-\\alpha_{t-1}}\\epsilon_{t-2}) + \\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\\\\n&= \\sqrt{\\alpha_t \\alpha_{t-1}}x_{t-2} + \\sqrt{\\alpha_t(1-\\alpha_{t-1})}\\epsilon_{t-2} + \\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\\\\n\\end{aligned}\n$$\n\nSince $\\epsilon_{t-2}$ and $\\epsilon_{t-1}$ are both independent and identically distributed as $\\mathcal{N}(0, I)$, according to the additivity property of Gaussian distributions:\n$$\\mathcal{N}(\\mu_1, \\sigma_1^2) + \\mathcal{N}(\\mu_2, \\sigma_2^2) = \\mathcal{N}(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)$$\nWe can combine the noise terms:\n$$\n\\sqrt{\\alpha_t(1-\\alpha_{t-1})}\\epsilon_{t-2} + \\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\sim \\mathcal{N}\\left(0, \\alpha_t(1-\\alpha_{t-1})I + (1-\\alpha_t)I\\right) = \\mathcal{N}\\left(0, (1 - \\alpha_t \\alpha_{t-1})I\\right)\n$$\nTherefore:\n$$\n\\begin{aligned}\nx_t &= \\sqrt{\\alpha_t \\alpha_{t-1}}x_{t-2} + \\sqrt{1 - \\alpha_t \\alpha_{t-1}}\\bar{\\epsilon}\\\\\n&=\\dots \\\\\n&= \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\mathbf{I}) \\\\\n\\end{aligned}\n$$\nwhere $\\alpha_t = 1 - \\beta_t$ and $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$.\n:::\n\n::: {.callout-note collapse=\"true\"}\n### What is reparameterization and Why use it?\nIn deep generative models, we often need to sample from a probability distribution that depends on some parameters. For example, in diffusion models, a neural network predicts the mean $\\mu$ and variance $\\sigma^2$ of a Gaussian distribution at each time step, and then we need to sample a variable $x_t$ from $\\mathcal{N}(\\mu, \\sigma^2)$.\n\nHowever, sampling directly from this distribution can make it difficult to compute gradients with respect to the parameters during training. Suppose our neural network has parameters $\\theta$, and we want to compute the gradient of some loss function $L$ with respect to $\\theta$. If we sample $x_t$ directly from $\\mathcal{N}(\\mu_\\theta, \\sigma^2_\\theta)$, the sampling operation is not a function, which introduces randomness that breaks the gradient flow, making it impossible to compute $\\nabla_\\theta L$. The fatal problem is that the sampling operation is not differentiable. \n\nThe core idea of the reparameterization trick is to seperate the randomness from the parameters by expressing the random variable $x_t$ as a deterministic function of the parameters and an independent noise variable. For a Gaussian distribution, we can express the sampling operation as:$$x_t = \\mu_\\theta + \\sigma_\\theta \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 1)$$\nHere, $\\epsilon \\sim \\mathcal{N}(0, 1)$, which is a random variable drawn from a standard normal distribution and independent of the parameters $\\theta$. This way, the randomness is isolated in $\\epsilon$, and the rest of the expression is a deterministic function of $\\theta$. It can be described by the following: $$x_{t} = \\mu_{\\theta} + \\sigma_{\\theta} \\odot \\epsilon$$\n\nNow, we can sample $x_t$ by sampling $\\epsilon$ from a fixed distribution (standard normal) and then applying the deterministic transformation. This allows us to compute gradients with respect to $\\theta$ during backpropagation, as the sampling operation is now differentiable with respect to the parameters, which $\\nabla_\\mu x_{t} = 1$ and $\\nabla_\\sigma x_{t} = \\epsilon$.\n\nWe also can compare two methods below in Pytorch:\n\n```python\n\nimport torch\n\nmu = torch.tensor([1.0], requires_grad=True)\nsigma = torch.tensor([0.5], requires_grad=True)\n# Direct sampling (not differentiable)\ndistribution = torch.distributions.Normal(mu, sigma)\nx_direct = distribution.sample()  # sample() for non-differentiable case\n# x_direct = distribution.rsample()  # rsample() for reparameterized case\nloss_direct = (x_direct - 2.0) ** 2\n# loss_direct.backward()  # This would fail to compute gradients, if using rsample(), it works\n\n# Reparameterization trick (differentiable)\nepsilon = torch.randn(1)\nx_reparam = mu + sigma * epsilon  # reparameterized sample\nloss_reparam = (x_reparam - 2.0) ** 2\nloss_reparam.backward()  # This works\nprint(mu.grad)  # Gradient with respect to mu\nprint(sigma.grad)  # Gradient with respect to sigma\n\n```\n:::\n\nWe can implement the forward process using PyTorch as follows:\n\n## Reverse diffusion process\nIn the process of forward diffusion, we defined how to gradually add Gaussian noise to the data using $q(x_t|x_{t-1})$. The reverse diffusion process aims to reverse this noising process, gradually removing the noise to recover the original data from pure noise by using its reverse conditional distribution $q(x_{t-1}|x_t)$. If we can accurately model this reverse process, we can start from random noise $x_T \\sim \\mathcal{N}(0, I)$ and iteratively sample $x_{T-1}, x_{T-2}, \\ldots, x_0$ to generate new data samples that resemble the original data distribution.\n\n### Can we directly compute the reverse conditional distribution？\nIn theory, if we have access to the true data distribution and the forward process, we can compute the reverse conditional distribution $q(x_{t-1}|x_t)$ using Bayes' theorem:\n$$q(x_{t-1}|x_t) = \\frac{q(x_t|x_{t-1})q(x_{t-1})}{q(x_t)}$$\nHowever, in practice, this is infeasible for several reasons:\n\n1. **Unknown Data Distribution**: We typically do not have access to the true data distribution $q(x - 1)$. The reason is that we only have a finite dataset of samples from the data distribution, not the distribution itself.\n\n2. **Intractable Integrals**: The denominator $q(x_t)$ is referred to as the marginal distribution of $x_t$. If we want to compute the probability at a specific time step $t$, we need to integrate over all possible previous states $x_{t-1}$:\n   $$q(x_t) = \\int q(x_t|x_{t-1})q(x_{t-1})dx_{t-1}$$\nThis integral is often intractable, especially in high-dimensional spaces, which is called the \"curse of dimensionality\".\n\nDue to these challenges, we cannot directly compute $q(x_{t-1}|x_t)$ and instead need to approximate it using a parameterized model, such as a neural network.\n\n### Using a neural network to approximate the reverse process\nWe can use a neural network with parameters $\\theta$ to approximate the reverse conditional distribution. When the diffusion step size $\\beta_t$ is sufficiently small, the reverse process $q(x_{t-1} \\mid x_t)$ also follows a Gaussian distribution. Therefore, we can define the neural network outputs as the parameters of this Gaussian distribution:\n$$ \np_\\theta(x_{0:T}) = p(x_T) \\prod_{t=1}^{T} p_\\theta(x_{t-1}|x_t) \\quad\np_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))\n$$\nwhere $\\mu_\\theta(x_t, t)$ and $\\Sigma_\\theta(x_t, t)$ are the mean and covariance predicted by the neural network for the reverse step from $x_t$ to $x_{t-1}$. \n\nWhat is more, in the paper @ho2020denoising, the authors find that using a fixed covariance $\\Sigma_\\theta(x_t, t) = \\beta_t I$ works well in practice, simplifying the model to only predict the mean $\\mu_\\theta(x_t, t)$, which is the main focus of the training process.\n\nNow, we can train the neural network to learn the parameters $\\theta$ such that the reverse process $p_\\theta(x_{t-1}|x_t)$ closely approximates the true but unknow reverse conditional distribution $q(x_{t-1}|x_t)$. To train the model, we aim to maximize the log-likelihood of the real data $x_0$ under the model, $\\log p_\\theta(x_0)$. Because direct likelihood optimization is intractable, we resort to Jensen's inequality and optimize the corresponding Evidence Lower Bound (ELBO):\n$$- \\log p_\\theta(x_0) \\le \\mathbb{E}_{q} \\left[ \\log \\frac{q(x_{1:T}|x_0)}{p_\\theta(x_{0:T})} \\right] = \\text{ELBO}$$\n\nTo convert each term in the equation to be analytically computable, the objective can be further rewritten to be a combination of several KL-divergence and entropy terms (See the detailed step-by-step process in @sohldickstein2015deepunsupervisedlearningusing):\n$$\\small \\text{ELBO} = \\mathbb{E}_{q} \\left[ \\underbrace{D_{KL}(q(x_T|x_0) || p_\\theta(x_T))}_{L_T}  + \\sum_{t=2}^{T} \\underbrace{D_{KL}(q(x_{t-1}|x_t, x_0) || p_\\theta(x_{t-1}|x_t))}_{L_{t-1}} \\underbrace{- \\log p_\\theta(x_0|x_1)}_{L_0} \\right]$$\nwhere $D_{KL}(P||Q)$ is the Kullback-Leibler divergence between two distributions $P$ and $Q$.\n\nAlthough the $q(x_{t-1}|x_t)$ is intractable, $q(x_{t-1}|x_t, x_0)$ is tractable because we condition on the original data point $x_0$. While $x_0$ is unknown at inference time, it is available during training as part of the dataset. Following the @eq-eq4 and @eq-eq5, we can derive the closed-form expression for $q(x_{t-1}|x_t, x_0)$ as a Gaussian distribution with mean $\\tilde{\\mu}_t(x_t, x_0)$ and variance $\\tilde{\\beta}_t I$.\n\n\nNow, our current goal is to have the neural network $p_\\theta \\sim \\mathcal{N}\\left(\\mu_\\theta,\\Sigma_\\theta\\right)$ approximate the true distribution $q(x_{t-1}|x_t, x_0) \\sim \\mathcal{N}\\left(\\tilde{\\mu}_t, \\tilde{\\beta}_t\\right)$. Since the parameter $\\Sigma_\\theta$ is often fixed as $\\beta_t I$, we mainly focus on training the neural network to predict the mean $\\textcolor{red}{\\mu_\\theta}$ to be as close as possible to the true mean $\\textcolor{red}{\\tilde{\\mu}_t = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_t \\right)}$.\n\nThe equation means that to accurately predict the mean $\\mu_\\theta$, the neural network only needs to predict the noise $\\epsilon_\\theta$ present in the current image.\n\nThis can be achieved by minimizing the KL-divergence between these two distributions:\n\n$$\n\\begin{aligned}\nL_{t} &= \\mathbb{E}_{x_0,\\epsilon} \\left[D_{KL}(q(x_{t-1}|x_t, x_0) || p_\\theta(x_{t-1}|x_t))\\right]\\\\\n&= \\mathbb{E}_{x_0,\\epsilon} \\left[\\frac{1}{2\\|\\Sigma_{\\theta}(x_t,t)\\|_2 ^ 2}\\| \\tilde{\\mu}_t(x_t, x_0) - \\mu_\\theta(x_t, t) \\|^2\\right]\\\\\n&= \\mathbb{E}_{x_0,\\epsilon} \\left[\\frac{1}{2\\|\\Sigma_{\\theta}(x_t,t)\\|_2 ^ 2}\\| \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_t \\right) - \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right) \\|^{2}\\right]\\\\\n&= \\mathbb{E}_{x_0,\\epsilon} \\left[\\frac{(1-\\alpha_t)^2}{2\\alpha_t(1-\\bar{\\alpha}_t)\\| \\Sigma_\\theta \\|_2^2}\n\\left\\lVert\n\\epsilon_t - \\epsilon_\\theta(x_t,t)\n\\right\\rVert^2\n\\right] \\\\\n&= \\mathbb{E}_{x_0,\\epsilon}\n\\left[\n\\frac{(1-\\alpha_t)^2}\n{2\\alpha_t(1-\\bar{\\alpha}_t)\\left\\lVert \\Sigma_\\theta \\right\\rVert _2^2}\n\\left\\lVert\n\\epsilon_t -\n\\epsilon_\\theta\\!\\left(\n\\sqrt{\\bar{\\alpha}_t} x_0 +\n\\sqrt{1-\\bar{\\alpha}_t}\\,\\epsilon_t,\nt\n\\right)\n\\right\\rVert^2\n\\right]\n\\end{aligned}\n$$\n\n\n@ho2020denoising further simplifies the loss function by removing the weighting term, leading to a more straightforward objective:\n\n$$\n\\boxed {\\textcolor{red}{L_t^{simple} = \\mathbb{E}_{x_0,\\epsilon,t} \\left[\\left\\lVert \\epsilon_t - \\epsilon_\\theta(x_t,t) \\right\\rVert^2\\right]}}\n$$\n\nThe final training objective is:\n\n$$\nL_{simple} = L_t^{simple} + C\n$$\n\nwhere $C$ is a constant that does not depend on the model parameters $\\theta$ and can be ignored during optimization.\n\n![](figures/DDPM-algo.png)\n\n### Sampling from the trained model\nOnce the neural network which can predict the noise $\\epsilon_\\theta(x_t, t)$ is trained, we can use it to sample new data points by reversing the diffusion process. Starting from pure Gaussian noise $x_T \\sim \\mathcal{N}(0, I)$, we iteratively apply the reverse diffusion steps using the learned model.\n\n$$\nx_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right) + \\sigma_t z\n$$\n\nwhere $z \\sim \\mathcal{N}(0, I)$ is random noise added at each step to maintain stochasticity, and $\\sigma_t$ is a scaling factor that can be adjusted based on the desired level of noise during sampling.\n\n::: {.callout-note}\n## Why we need to add $\\sigma_t z$？\nThis term corresponds to Langevin dynamics. Without it, the sampling process becomes a deterministic numerical procedure, which often leads to overly smooth images with a lack of fine details. Introducing stochastic perturbations helps correct estimation errors at each step and enables the generative model to better explore the true image manifold.\n\n:::\n\n### How to train the model\nWe get the simplified loss function as follows:\n\n$$\nL_{simple} = \\left\\|\\epsilon_t - \\epsilon_\\theta(x_t,t) \\right\\|^2\n$$\n\nThis means that the training procedure consists of only four steps:\n\n1. **Random sampling**: Randomly sample a data point $x_0$ from the training dataset.\n\n2. **Random timestep**: Select a random time step $ t \\in [1, T]$.\n\n3. **Add noise in the forward process**: Generate a noisy version of the data point $x_t$ using the forward diffusion equation:\n\n   $$ x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0, I)$$\n\n4. **Train the model**: Use the noisy image $x_t$ and the corresponding noise $\\epsilon_t$ to train the neural network $\\epsilon_\\theta(x_t, t)$ to predict the noise.\n\n5. **Loss computation**: Compute the loss using the simplified loss function:\n\n   $$ L_{simple} = \\left\\|\\epsilon_t - \\epsilon_\\theta(x_t,t) \\right\\|^2 $$\n   \n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css","styles.css"],"toc":true,"toc-depth":3,"highlight-style":"atom-one","output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.26","theme":["cosmo","brand"],"math":"katex","title-block-banner":true,"title":"Diffusion Model","author":"Yuyang","date":"2026-01-09","categories":["Deep Learning","Math"],"toc-location":"right","bibliography":["references.bib"],"link-citations":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}